{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in links: https://download.pytorch.org/whl/cpu/torch_stable.html\n",
      "Requirement already satisfied: torch==1.0.1 in /opt/conda/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: torchvision==0.2.2 in /opt/conda/lib/python3.7/site-packages (0.2.2)\n",
      "Requirement already satisfied: tqdm==4.19.9 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.2.2) (4.19.9)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.2.2) (9.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchvision==0.2.2) (1.14.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.2.2) (1.21.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.0.1 torchvision==0.2.2 -f https://download.pytorch.org/whl/cpu/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (1.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proprocess the data, create kg etc. \n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import data_utils\n",
    "import utils\n",
    "\n",
    "from utils import *\n",
    "from data_utils import ChallengeDataset, ChallengeDataLoader\n",
    "from knowledge_graph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load challenge dataset from file...\n",
      "Load user of size 1000\n",
      "Load article of size 71\n",
      "Load word of size 2246\n",
      "Load related_article of size 71\n",
      "Load topic of size 55\n",
      "Load product of size 64\n",
      "Load topic_tag of size 24\n",
      "Load product_tag of size 45\n",
      "Load has_topic of size 71\n",
      "Load has_product of size 71\n",
      "Load also_response of size 71\n",
      "Load recommended_together of size 71\n",
      "Load response_together of size 71\n",
      "Load has_topic_tag of size 71\n",
      "Load has_product_tag of size 71\n",
      "Load text of size 70294 word count= 28022586\n",
      "Create word sampling rate\n",
      "Create challenge knowledge graph from dataset...\n",
      "Load entities...\n",
      "Total 3576 nodes.\n",
      "Load text...\n",
      "Total 333282 text edges.\n",
      "Load knowledge has_topic...\n",
      "Total 566 has_topic edges.\n",
      "Load knowledge has_product...\n",
      "Total 720 has_product edges.\n",
      "Load knowledge also_response...\n",
      "Total 7334 also_response edges.\n",
      "Load knowledge recommended_together...\n",
      "Total 368 recommended_together edges.\n",
      "Load knowledge response_together...\n",
      "Total 142 response_together edges.\n",
      "Load knowledge has_topic_tag...\n",
      "Total 132 has_topic_tag edges.\n",
      "Load knowledge has_product_tag...\n",
      "Total 284 has_product_tag edges.\n",
      "Remove duplicates...\n",
      "Compute node degrees...\n",
      "Generate challenge train/test labels.\n"
     ]
    }
   ],
   "source": [
    "def generate_labels(dataset, mode='train'):\n",
    "    review_file = '{}/{}.txt'.format(DATASET_DIR[dataset], mode)\n",
    "    user_articles = {}  # {uid: [aid,...], ...}\n",
    "    with open(review_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            arr = line.split('\\t')\n",
    "            user_idx = int(arr[0])\n",
    "            article_idx = int(arr[1])\n",
    "            if user_idx not in user_articles:\n",
    "                user_articles[user_idx] = []\n",
    "            user_articles[user_idx].append(article_idx)\n",
    "    save_labels(dataset, user_articles, mode=mode)\n",
    "\n",
    "\n",
    "def main(data):\n",
    "    dataset_n = data\n",
    "\n",
    "    # Create Dataset instance for dataset.\n",
    "    # ========== BEGIN ========== #\n",
    "    print('Load', dataset_n, 'dataset from file...')\n",
    "    if not os.path.isdir(TMP_DIR[dataset_n]):\n",
    "        os.makedirs(TMP_DIR[dataset_n])\n",
    "    dataset = ChallengeDataset(DATASET_DIR[dataset_n])\n",
    "    save_dataset(dataset_n, dataset)\n",
    "\n",
    "    # Generate knowledge graph instance.\n",
    "    # ========== BEGIN ========== #\n",
    "    \n",
    "    print('Create', dataset_n, 'knowledge graph from dataset...')\n",
    "    dataset = load_dataset(dataset_n)\n",
    "    kg = KnowledgeGraph(dataset)\n",
    "    kg.compute_degrees()\n",
    "    save_kg(dataset_n, kg)\n",
    "    # =========== END =========== #\n",
    "    \n",
    "    # Genereate train/test labels.\n",
    "    # ========== BEGIN ========== #\n",
    "    print('Generate', dataset_n, 'train/test labels.')\n",
    "    generate_labels(dataset_n, 'train')\n",
    "    generate_labels(dataset_n, 'test')\n",
    "    # =========== END =========== #\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transe_model import KnowledgeEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp/Challenge_Dataset/train_transe_model\n",
      "[INFO]  Epoch: 01 | Words: 30451/28022587 | Lr: 0.00500 | Smooth loss: 38.73018\n",
      "[INFO]  Epoch: 01 | Words: 61084/28022587 | Lr: 0.00500 | Smooth loss: 37.78879\n",
      "[INFO]  Epoch: 01 | Words: 90328/28022587 | Lr: 0.00500 | Smooth loss: 36.85461\n",
      "[INFO]  Epoch: 01 | Words: 121101/28022587 | Lr: 0.00500 | Smooth loss: 36.04648\n",
      "[INFO]  Epoch: 01 | Words: 151713/28022587 | Lr: 0.00500 | Smooth loss: 35.67448\n",
      "[INFO]  Epoch: 01 | Words: 183357/28022587 | Lr: 0.00500 | Smooth loss: 35.34737\n",
      "[INFO]  Epoch: 01 | Words: 213026/28022587 | Lr: 0.00500 | Smooth loss: 34.99123\n",
      "[INFO]  Epoch: 01 | Words: 242182/28022587 | Lr: 0.00500 | Smooth loss: 34.03343\n",
      "[INFO]  Epoch: 01 | Words: 273517/28022587 | Lr: 0.00500 | Smooth loss: 34.32667\n",
      "[INFO]  Epoch: 01 | Words: 303758/28022587 | Lr: 0.00500 | Smooth loss: 34.00162\n",
      "[INFO]  Epoch: 01 | Words: 333455/28022587 | Lr: 0.00500 | Smooth loss: 33.07016\n",
      "[INFO]  Epoch: 01 | Words: 364129/28022587 | Lr: 0.00500 | Smooth loss: 32.56898\n",
      "[INFO]  Epoch: 01 | Words: 393086/28022587 | Lr: 0.00500 | Smooth loss: 32.82930\n",
      "[INFO]  Epoch: 01 | Words: 423409/28022587 | Lr: 0.00500 | Smooth loss: 32.66973\n",
      "[INFO]  Epoch: 01 | Words: 452644/28022587 | Lr: 0.00500 | Smooth loss: 31.98271\n",
      "[INFO]  Epoch: 01 | Words: 482327/28022587 | Lr: 0.00500 | Smooth loss: 31.67653\n",
      "[INFO]  Epoch: 01 | Words: 513166/28022587 | Lr: 0.00500 | Smooth loss: 30.81095\n",
      "[INFO]  Epoch: 01 | Words: 543009/28022587 | Lr: 0.00500 | Smooth loss: 30.65507\n",
      "[INFO]  Epoch: 01 | Words: 573955/28022587 | Lr: 0.00500 | Smooth loss: 30.33141\n",
      "[INFO]  Epoch: 01 | Words: 603970/28022587 | Lr: 0.00500 | Smooth loss: 29.23267\n",
      "[INFO]  Epoch: 01 | Words: 634593/28022587 | Lr: 0.00500 | Smooth loss: 28.96741\n",
      "[INFO]  Epoch: 01 | Words: 665581/28022587 | Lr: 0.00500 | Smooth loss: 29.17053\n",
      "[INFO]  Epoch: 01 | Words: 695673/28022587 | Lr: 0.00500 | Smooth loss: 28.50164\n",
      "[INFO]  Epoch: 01 | Words: 726750/28022587 | Lr: 0.00500 | Smooth loss: 28.33650\n",
      "[INFO]  Epoch: 01 | Words: 757240/28022587 | Lr: 0.00500 | Smooth loss: 28.29109\n",
      "[INFO]  Epoch: 01 | Words: 786281/28022587 | Lr: 0.00500 | Smooth loss: 27.77531\n",
      "[INFO]  Epoch: 01 | Words: 816365/28022587 | Lr: 0.00500 | Smooth loss: 27.64150\n",
      "[INFO]  Epoch: 01 | Words: 846904/28022587 | Lr: 0.00500 | Smooth loss: 27.78614\n",
      "[INFO]  Epoch: 01 | Words: 876987/28022587 | Lr: 0.00500 | Smooth loss: 27.47997\n",
      "[INFO]  Epoch: 01 | Words: 907306/28022587 | Lr: 0.00500 | Smooth loss: 27.72992\n",
      "[INFO]  Epoch: 01 | Words: 937977/28022587 | Lr: 0.00500 | Smooth loss: 27.00089\n",
      "[INFO]  Epoch: 01 | Words: 968076/28022587 | Lr: 0.00500 | Smooth loss: 26.35646\n",
      "[INFO]  Epoch: 01 | Words: 998868/28022587 | Lr: 0.00500 | Smooth loss: 27.24026\n",
      "[INFO]  Epoch: 01 | Words: 1029238/28022587 | Lr: 0.00500 | Smooth loss: 27.12275\n",
      "[INFO]  Epoch: 01 | Words: 1059750/28022587 | Lr: 0.00500 | Smooth loss: 27.22420\n",
      "[INFO]  Epoch: 01 | Words: 1089755/28022587 | Lr: 0.00500 | Smooth loss: 27.08346\n",
      "[INFO]  Epoch: 01 | Words: 1120013/28022587 | Lr: 0.00500 | Smooth loss: 27.12473\n",
      "[INFO]  Epoch: 01 | Words: 1149488/28022587 | Lr: 0.00500 | Smooth loss: 27.07326\n",
      "[INFO]  Epoch: 01 | Words: 1179448/28022587 | Lr: 0.00500 | Smooth loss: 26.68078\n",
      "[INFO]  Epoch: 01 | Words: 1208948/28022587 | Lr: 0.00500 | Smooth loss: 26.44169\n",
      "[INFO]  Epoch: 01 | Words: 1239280/28022587 | Lr: 0.00500 | Smooth loss: 26.75856\n",
      "[INFO]  Epoch: 01 | Words: 1269401/28022587 | Lr: 0.00500 | Smooth loss: 27.18138\n",
      "[INFO]  Epoch: 01 | Words: 1300363/28022587 | Lr: 0.00500 | Smooth loss: 26.61374\n",
      "[INFO]  Epoch: 01 | Words: 1331474/28022587 | Lr: 0.00500 | Smooth loss: 26.31292\n",
      "[INFO]  Epoch: 01 | Words: 1362172/28022587 | Lr: 0.00500 | Smooth loss: 26.44977\n",
      "[INFO]  Epoch: 01 | Words: 1392835/28022587 | Lr: 0.00500 | Smooth loss: 25.70848\n",
      "[INFO]  Epoch: 01 | Words: 1423355/28022587 | Lr: 0.00500 | Smooth loss: 26.49418\n",
      "[INFO]  Epoch: 01 | Words: 1452714/28022587 | Lr: 0.00500 | Smooth loss: 25.83283\n",
      "[INFO]  Epoch: 01 | Words: 1481772/28022587 | Lr: 0.00500 | Smooth loss: 25.55115\n",
      "[INFO]  Epoch: 01 | Words: 1512469/28022587 | Lr: 0.00500 | Smooth loss: 26.09256\n",
      "[INFO]  Epoch: 01 | Words: 1540817/28022587 | Lr: 0.00500 | Smooth loss: 25.74038\n",
      "[INFO]  Epoch: 01 | Words: 1570993/28022587 | Lr: 0.00500 | Smooth loss: 25.73906\n",
      "[INFO]  Epoch: 01 | Words: 1601043/28022587 | Lr: 0.00500 | Smooth loss: 25.57046\n",
      "[INFO]  Epoch: 01 | Words: 1631221/28022587 | Lr: 0.00500 | Smooth loss: 25.81850\n",
      "[INFO]  Epoch: 01 | Words: 1661365/28022587 | Lr: 0.00500 | Smooth loss: 25.93086\n",
      "[INFO]  Epoch: 01 | Words: 1692137/28022587 | Lr: 0.00500 | Smooth loss: 25.67106\n",
      "[INFO]  Epoch: 01 | Words: 1722885/28022587 | Lr: 0.00500 | Smooth loss: 25.67751\n",
      "[INFO]  Epoch: 01 | Words: 1752431/28022587 | Lr: 0.00500 | Smooth loss: 25.35666\n",
      "[INFO]  Epoch: 01 | Words: 1784008/28022587 | Lr: 0.00500 | Smooth loss: 25.79151\n",
      "[INFO]  Epoch: 01 | Words: 1814274/28022587 | Lr: 0.00500 | Smooth loss: 25.19987\n",
      "[INFO]  Epoch: 01 | Words: 1844074/28022587 | Lr: 0.00500 | Smooth loss: 24.68233\n",
      "[INFO]  Epoch: 01 | Words: 1873868/28022587 | Lr: 0.00500 | Smooth loss: 25.15313\n",
      "[INFO]  Epoch: 01 | Words: 1903710/28022587 | Lr: 0.00500 | Smooth loss: 25.25225\n",
      "[INFO]  Epoch: 01 | Words: 1933502/28022587 | Lr: 0.00500 | Smooth loss: 25.93748\n",
      "[INFO]  Epoch: 01 | Words: 1963978/28022587 | Lr: 0.00500 | Smooth loss: 24.40221\n",
      "[INFO]  Epoch: 01 | Words: 1994954/28022587 | Lr: 0.00500 | Smooth loss: 24.89167\n",
      "[INFO]  Epoch: 01 | Words: 2025728/28022587 | Lr: 0.00500 | Smooth loss: 25.14700\n",
      "[INFO]  Epoch: 01 | Words: 2055706/28022587 | Lr: 0.00500 | Smooth loss: 24.73926\n",
      "[INFO]  Epoch: 01 | Words: 2086436/28022587 | Lr: 0.00500 | Smooth loss: 25.62578\n",
      "[INFO]  Epoch: 01 | Words: 2117209/28022587 | Lr: 0.00500 | Smooth loss: 25.15437\n",
      "[INFO]  Epoch: 01 | Words: 2147184/28022587 | Lr: 0.00500 | Smooth loss: 24.91445\n",
      "[INFO]  Epoch: 01 | Words: 2177620/28022587 | Lr: 0.00500 | Smooth loss: 23.37847\n",
      "[INFO]  Epoch: 01 | Words: 2208761/28022587 | Lr: 0.00500 | Smooth loss: 24.37601\n",
      "[INFO]  Epoch: 01 | Words: 2239389/28022587 | Lr: 0.00500 | Smooth loss: 23.77639\n",
      "[INFO]  Epoch: 01 | Words: 2269393/28022587 | Lr: 0.00500 | Smooth loss: 25.08482\n",
      "[INFO]  Epoch: 01 | Words: 2300347/28022587 | Lr: 0.00500 | Smooth loss: 24.70279\n",
      "[INFO]  Epoch: 01 | Words: 2331329/28022587 | Lr: 0.00500 | Smooth loss: 24.33535\n",
      "[INFO]  Epoch: 01 | Words: 2360918/28022587 | Lr: 0.00500 | Smooth loss: 24.01485\n",
      "[INFO]  Epoch: 01 | Words: 2391025/28022587 | Lr: 0.00500 | Smooth loss: 24.18419\n",
      "[INFO]  Epoch: 01 | Words: 2421905/28022587 | Lr: 0.00500 | Smooth loss: 23.79131\n",
      "[INFO]  Epoch: 01 | Words: 2452264/28022587 | Lr: 0.00500 | Smooth loss: 23.87884\n",
      "[INFO]  Epoch: 01 | Words: 2482517/28022587 | Lr: 0.00500 | Smooth loss: 23.92027\n",
      "[INFO]  Epoch: 01 | Words: 2513308/28022587 | Lr: 0.00500 | Smooth loss: 23.28900\n",
      "[INFO]  Epoch: 01 | Words: 2543252/28022587 | Lr: 0.00500 | Smooth loss: 23.60223\n",
      "[INFO]  Epoch: 01 | Words: 2574189/28022587 | Lr: 0.00500 | Smooth loss: 22.56941\n",
      "[INFO]  Epoch: 01 | Words: 2604284/28022587 | Lr: 0.00500 | Smooth loss: 23.34302\n",
      "[INFO]  Epoch: 01 | Words: 2633658/28022587 | Lr: 0.00500 | Smooth loss: 24.74286\n",
      "[INFO]  Epoch: 01 | Words: 2664385/28022587 | Lr: 0.00500 | Smooth loss: 23.54499\n",
      "[INFO]  Epoch: 01 | Words: 2694509/28022587 | Lr: 0.00500 | Smooth loss: 22.36315\n",
      "[INFO]  Epoch: 01 | Words: 2725343/28022587 | Lr: 0.00500 | Smooth loss: 23.86467\n",
      "[INFO]  Epoch: 01 | Words: 2755696/28022587 | Lr: 0.00500 | Smooth loss: 22.91852\n",
      "[INFO]  Epoch: 01 | Words: 2785944/28022587 | Lr: 0.00500 | Smooth loss: 24.50916\n",
      "[INFO]  Epoch: 01 | Words: 2816237/28022587 | Lr: 0.00500 | Smooth loss: 23.35897\n",
      "[INFO]  Epoch: 01 | Words: 2846843/28022587 | Lr: 0.00500 | Smooth loss: 23.43143\n",
      "[INFO]  Epoch: 01 | Words: 2877806/28022587 | Lr: 0.00500 | Smooth loss: 23.86453\n",
      "[INFO]  Epoch: 01 | Words: 2907397/28022587 | Lr: 0.00500 | Smooth loss: 23.08085\n",
      "[INFO]  Epoch: 01 | Words: 2936699/28022587 | Lr: 0.00500 | Smooth loss: 23.58558\n",
      "[INFO]  Epoch: 01 | Words: 2966411/28022587 | Lr: 0.00500 | Smooth loss: 23.52935\n",
      "[INFO]  Epoch: 01 | Words: 2995979/28022587 | Lr: 0.00500 | Smooth loss: 23.39624\n",
      "[INFO]  Epoch: 01 | Words: 3027052/28022587 | Lr: 0.00500 | Smooth loss: 23.37295\n",
      "[INFO]  Epoch: 01 | Words: 3058089/28022587 | Lr: 0.00500 | Smooth loss: 22.68377\n",
      "[INFO]  Epoch: 01 | Words: 3087714/28022587 | Lr: 0.00500 | Smooth loss: 23.24116\n",
      "[INFO]  Epoch: 01 | Words: 3117636/28022587 | Lr: 0.00500 | Smooth loss: 22.73892\n",
      "[INFO]  Epoch: 01 | Words: 3147045/28022587 | Lr: 0.00500 | Smooth loss: 22.53817\n",
      "[INFO]  Epoch: 01 | Words: 3178120/28022587 | Lr: 0.00500 | Smooth loss: 22.95758\n",
      "[INFO]  Epoch: 01 | Words: 3209287/28022587 | Lr: 0.00500 | Smooth loss: 23.43124\n",
      "[INFO]  Epoch: 01 | Words: 3238899/28022587 | Lr: 0.00500 | Smooth loss: 23.55279\n",
      "[INFO]  Epoch: 01 | Words: 3270556/28022587 | Lr: 0.00500 | Smooth loss: 23.06787\n",
      "[INFO]  Epoch: 01 | Words: 3299591/28022587 | Lr: 0.00500 | Smooth loss: 23.23108\n",
      "[INFO]  Epoch: 01 | Words: 3330068/28022587 | Lr: 0.00500 | Smooth loss: 23.48738\n",
      "[INFO]  Epoch: 01 | Words: 3360743/28022587 | Lr: 0.00500 | Smooth loss: 22.57201\n",
      "[INFO]  Epoch: 01 | Words: 3390679/28022587 | Lr: 0.00500 | Smooth loss: 22.39347\n",
      "[INFO]  Epoch: 01 | Words: 3420820/28022587 | Lr: 0.00500 | Smooth loss: 22.91820\n",
      "[INFO]  Epoch: 01 | Words: 3451819/28022587 | Lr: 0.00500 | Smooth loss: 23.85863\n",
      "[INFO]  Epoch: 01 | Words: 3481594/28022587 | Lr: 0.00500 | Smooth loss: 22.62473\n",
      "[INFO]  Epoch: 01 | Words: 3511708/28022587 | Lr: 0.00500 | Smooth loss: 22.37876\n",
      "[INFO]  Epoch: 01 | Words: 3542020/28022587 | Lr: 0.00500 | Smooth loss: 22.27347\n",
      "[INFO]  Epoch: 01 | Words: 3572568/28022587 | Lr: 0.00500 | Smooth loss: 22.75540\n",
      "[INFO]  Epoch: 01 | Words: 3602550/28022587 | Lr: 0.00500 | Smooth loss: 22.46185\n",
      "[INFO]  Epoch: 01 | Words: 3634084/28022587 | Lr: 0.00500 | Smooth loss: 23.11251\n",
      "[INFO]  Epoch: 01 | Words: 3664338/28022587 | Lr: 0.00500 | Smooth loss: 22.26514\n",
      "[INFO]  Epoch: 01 | Words: 3694871/28022587 | Lr: 0.00500 | Smooth loss: 22.19233\n",
      "[INFO]  Epoch: 01 | Words: 3724991/28022587 | Lr: 0.00500 | Smooth loss: 22.95649\n",
      "[INFO]  Epoch: 01 | Words: 3754906/28022587 | Lr: 0.00500 | Smooth loss: 22.44800\n",
      "[INFO]  Epoch: 01 | Words: 3786022/28022587 | Lr: 0.00500 | Smooth loss: 22.35596\n",
      "[INFO]  Epoch: 01 | Words: 3816302/28022587 | Lr: 0.00500 | Smooth loss: 21.70606\n",
      "[INFO]  Epoch: 01 | Words: 3846049/28022587 | Lr: 0.00500 | Smooth loss: 22.27401\n",
      "[INFO]  Epoch: 01 | Words: 3877363/28022587 | Lr: 0.00500 | Smooth loss: 22.13622\n",
      "[INFO]  Epoch: 01 | Words: 3909046/28022587 | Lr: 0.00500 | Smooth loss: 23.16511\n",
      "[INFO]  Epoch: 01 | Words: 3939520/28022587 | Lr: 0.00500 | Smooth loss: 23.03216\n",
      "[INFO]  Epoch: 01 | Words: 3969540/28022587 | Lr: 0.00500 | Smooth loss: 21.66639\n",
      "[INFO]  Epoch: 01 | Words: 3999459/28022587 | Lr: 0.00500 | Smooth loss: 22.76576\n",
      "[INFO]  Epoch: 01 | Words: 4030486/28022587 | Lr: 0.00500 | Smooth loss: 23.07173\n",
      "[INFO]  Epoch: 01 | Words: 4059973/28022587 | Lr: 0.00500 | Smooth loss: 22.94036\n",
      "[INFO]  Epoch: 01 | Words: 4090552/28022587 | Lr: 0.00500 | Smooth loss: 22.15163\n",
      "[INFO]  Epoch: 01 | Words: 4121331/28022587 | Lr: 0.00500 | Smooth loss: 22.44645\n",
      "[INFO]  Epoch: 01 | Words: 4152156/28022587 | Lr: 0.00500 | Smooth loss: 22.37032\n",
      "[INFO]  Epoch: 01 | Words: 4181264/28022587 | Lr: 0.00500 | Smooth loss: 20.83310\n",
      "[INFO]  Epoch: 01 | Words: 4211957/28022587 | Lr: 0.00500 | Smooth loss: 22.75884\n",
      "[INFO]  Epoch: 01 | Words: 4243091/28022587 | Lr: 0.00500 | Smooth loss: 21.59756\n",
      "[INFO]  Epoch: 01 | Words: 4274443/28022587 | Lr: 0.00500 | Smooth loss: 21.35867\n",
      "[INFO]  Epoch: 01 | Words: 4305385/28022587 | Lr: 0.00500 | Smooth loss: 22.59768\n",
      "[INFO]  Epoch: 01 | Words: 4335064/28022587 | Lr: 0.00500 | Smooth loss: 22.72398\n",
      "[INFO]  Epoch: 01 | Words: 4365613/28022587 | Lr: 0.00500 | Smooth loss: 22.84458\n",
      "[INFO]  Epoch: 01 | Words: 4395702/28022587 | Lr: 0.00500 | Smooth loss: 21.42069\n",
      "[INFO]  Epoch: 01 | Words: 4425606/28022587 | Lr: 0.00500 | Smooth loss: 22.38530\n",
      "[INFO]  Epoch: 01 | Words: 4455927/28022587 | Lr: 0.00500 | Smooth loss: 23.00805\n",
      "[INFO]  Epoch: 01 | Words: 4486381/28022587 | Lr: 0.00500 | Smooth loss: 21.80772\n",
      "[INFO]  Epoch: 01 | Words: 4515346/28022587 | Lr: 0.00500 | Smooth loss: 21.58433\n",
      "[INFO]  Epoch: 01 | Words: 4546255/28022587 | Lr: 0.00500 | Smooth loss: 22.23963\n",
      "[INFO]  Epoch: 01 | Words: 4576161/28022587 | Lr: 0.00500 | Smooth loss: 22.40370\n",
      "[INFO]  Epoch: 01 | Words: 4606428/28022587 | Lr: 0.00500 | Smooth loss: 21.21674\n",
      "[INFO]  Epoch: 01 | Words: 4636094/28022587 | Lr: 0.00500 | Smooth loss: 21.14778\n",
      "[INFO]  Epoch: 01 | Words: 4667415/28022587 | Lr: 0.00500 | Smooth loss: 23.02453\n",
      "[INFO]  Epoch: 01 | Words: 4697600/28022587 | Lr: 0.00500 | Smooth loss: 21.32247\n",
      "[INFO]  Epoch: 01 | Words: 4727994/28022587 | Lr: 0.00500 | Smooth loss: 21.92047\n",
      "[INFO]  Epoch: 01 | Words: 4758276/28022587 | Lr: 0.00500 | Smooth loss: 22.20848\n",
      "[INFO]  Epoch: 01 | Words: 4787989/28022587 | Lr: 0.00500 | Smooth loss: 21.64574\n",
      "[INFO]  Epoch: 01 | Words: 4818623/28022587 | Lr: 0.00500 | Smooth loss: 21.85107\n",
      "[INFO]  Epoch: 01 | Words: 4848493/28022587 | Lr: 0.00500 | Smooth loss: 21.41065\n",
      "[INFO]  Epoch: 01 | Words: 4878311/28022587 | Lr: 0.00500 | Smooth loss: 21.92966\n",
      "[INFO]  Epoch: 01 | Words: 4909216/28022587 | Lr: 0.00500 | Smooth loss: 21.33532\n",
      "[INFO]  Epoch: 01 | Words: 4939585/28022587 | Lr: 0.00500 | Smooth loss: 21.69250\n",
      "[INFO]  Epoch: 01 | Words: 4969788/28022587 | Lr: 0.00500 | Smooth loss: 21.55112\n",
      "[INFO]  Epoch: 01 | Words: 5000585/28022587 | Lr: 0.00500 | Smooth loss: 21.50493\n",
      "[INFO]  Epoch: 01 | Words: 5031527/28022587 | Lr: 0.00500 | Smooth loss: 21.85630\n",
      "[INFO]  Epoch: 01 | Words: 5063123/28022587 | Lr: 0.00500 | Smooth loss: 21.51592\n",
      "[INFO]  Epoch: 01 | Words: 5094367/28022587 | Lr: 0.00500 | Smooth loss: 22.00759\n",
      "[INFO]  Epoch: 01 | Words: 5124697/28022587 | Lr: 0.00500 | Smooth loss: 21.17274\n",
      "[INFO]  Epoch: 01 | Words: 5154778/28022587 | Lr: 0.00500 | Smooth loss: 21.04810\n",
      "[INFO]  Epoch: 01 | Words: 5186173/28022587 | Lr: 0.00500 | Smooth loss: 21.25012\n",
      "[INFO]  Epoch: 01 | Words: 5216551/28022587 | Lr: 0.00500 | Smooth loss: 21.49579\n",
      "[INFO]  Epoch: 01 | Words: 5247631/28022587 | Lr: 0.00500 | Smooth loss: 21.68668\n",
      "[INFO]  Epoch: 01 | Words: 5277416/28022587 | Lr: 0.00500 | Smooth loss: 22.32926\n",
      "[INFO]  Epoch: 01 | Words: 5307270/28022587 | Lr: 0.00500 | Smooth loss: 21.40560\n",
      "[INFO]  Epoch: 01 | Words: 5338623/28022587 | Lr: 0.00500 | Smooth loss: 21.95146\n",
      "[INFO]  Epoch: 01 | Words: 5367732/28022587 | Lr: 0.00500 | Smooth loss: 21.16871\n",
      "[INFO]  Epoch: 01 | Words: 5398490/28022587 | Lr: 0.00500 | Smooth loss: 21.75783\n",
      "[INFO]  Epoch: 01 | Words: 5430120/28022587 | Lr: 0.00500 | Smooth loss: 21.69828\n",
      "[INFO]  Epoch: 01 | Words: 5461940/28022587 | Lr: 0.00500 | Smooth loss: 20.85658\n",
      "[INFO]  Epoch: 01 | Words: 5492699/28022587 | Lr: 0.00500 | Smooth loss: 21.74585\n",
      "[INFO]  Epoch: 01 | Words: 5523170/28022587 | Lr: 0.00500 | Smooth loss: 20.92007\n",
      "[INFO]  Epoch: 01 | Words: 5553545/28022587 | Lr: 0.00500 | Smooth loss: 21.77095\n",
      "[INFO]  Epoch: 01 | Words: 5583207/28022587 | Lr: 0.00500 | Smooth loss: 20.97694\n",
      "[INFO]  Epoch: 01 | Words: 5612782/28022587 | Lr: 0.00500 | Smooth loss: 20.89282\n",
      "[INFO]  Epoch: 01 | Words: 5642392/28022587 | Lr: 0.00500 | Smooth loss: 21.28117\n",
      "[INFO]  Epoch: 01 | Words: 5672899/28022587 | Lr: 0.00500 | Smooth loss: 21.60117\n",
      "[INFO]  Epoch: 01 | Words: 5702810/28022587 | Lr: 0.00500 | Smooth loss: 22.56648\n",
      "[INFO]  Epoch: 01 | Words: 5732775/28022587 | Lr: 0.00500 | Smooth loss: 20.98346\n",
      "[INFO]  Epoch: 01 | Words: 5762724/28022587 | Lr: 0.00500 | Smooth loss: 20.72810\n",
      "[INFO]  Epoch: 01 | Words: 5792363/28022587 | Lr: 0.00500 | Smooth loss: 21.39000\n",
      "[INFO]  Epoch: 01 | Words: 5822734/28022587 | Lr: 0.00500 | Smooth loss: 21.72579\n",
      "[INFO]  Epoch: 01 | Words: 5851896/28022587 | Lr: 0.00500 | Smooth loss: 21.71338\n",
      "[INFO]  Epoch: 01 | Words: 5881508/28022587 | Lr: 0.00500 | Smooth loss: 21.28274\n",
      "[INFO]  Epoch: 01 | Words: 5912139/28022587 | Lr: 0.00500 | Smooth loss: 21.97699\n",
      "[INFO]  Epoch: 01 | Words: 5942275/28022587 | Lr: 0.00500 | Smooth loss: 20.79913\n",
      "[INFO]  Epoch: 01 | Words: 5973234/28022587 | Lr: 0.00500 | Smooth loss: 21.78178\n",
      "[INFO]  Epoch: 01 | Words: 6002707/28022587 | Lr: 0.00500 | Smooth loss: 22.11209\n",
      "[INFO]  Epoch: 01 | Words: 6032391/28022587 | Lr: 0.00500 | Smooth loss: 21.36043\n",
      "[INFO]  Epoch: 01 | Words: 6061731/28022587 | Lr: 0.00500 | Smooth loss: 20.57386\n",
      "[INFO]  Epoch: 01 | Words: 6091848/28022587 | Lr: 0.00500 | Smooth loss: 20.96154\n",
      "[INFO]  Epoch: 01 | Words: 6121010/28022587 | Lr: 0.00500 | Smooth loss: 20.05468\n",
      "[INFO]  Epoch: 01 | Words: 6150323/28022587 | Lr: 0.00500 | Smooth loss: 20.14017\n",
      "[INFO]  Epoch: 01 | Words: 6181029/28022587 | Lr: 0.00500 | Smooth loss: 20.94492\n",
      "[INFO]  Epoch: 01 | Words: 6211640/28022587 | Lr: 0.00500 | Smooth loss: 20.78974\n",
      "[INFO]  Epoch: 01 | Words: 6242168/28022587 | Lr: 0.00500 | Smooth loss: 21.08963\n",
      "[INFO]  Epoch: 01 | Words: 6272681/28022587 | Lr: 0.00500 | Smooth loss: 21.21271\n",
      "[INFO]  Epoch: 01 | Words: 6301930/28022587 | Lr: 0.00500 | Smooth loss: 21.49437\n",
      "[INFO]  Epoch: 01 | Words: 6330937/28022587 | Lr: 0.00500 | Smooth loss: 19.98096\n",
      "[INFO]  Epoch: 01 | Words: 6360524/28022587 | Lr: 0.00500 | Smooth loss: 21.29323\n",
      "[INFO]  Epoch: 01 | Words: 6390024/28022587 | Lr: 0.00500 | Smooth loss: 20.48773\n",
      "[INFO]  Epoch: 01 | Words: 6421067/28022587 | Lr: 0.00500 | Smooth loss: 21.53703\n",
      "[INFO]  Epoch: 01 | Words: 6452375/28022587 | Lr: 0.00500 | Smooth loss: 21.66702\n",
      "[INFO]  Epoch: 01 | Words: 6481933/28022587 | Lr: 0.00500 | Smooth loss: 20.26881\n",
      "[INFO]  Epoch: 01 | Words: 6512117/28022587 | Lr: 0.00500 | Smooth loss: 20.60355\n",
      "[INFO]  Epoch: 01 | Words: 6541303/28022587 | Lr: 0.00500 | Smooth loss: 19.97465\n",
      "[INFO]  Epoch: 01 | Words: 6571388/28022587 | Lr: 0.00500 | Smooth loss: 21.04949\n",
      "[INFO]  Epoch: 01 | Words: 6601333/28022587 | Lr: 0.00500 | Smooth loss: 20.51088\n",
      "[INFO]  Epoch: 01 | Words: 6632619/28022587 | Lr: 0.00500 | Smooth loss: 20.13264\n",
      "[INFO]  Epoch: 01 | Words: 6663976/28022587 | Lr: 0.00500 | Smooth loss: 21.08734\n",
      "[INFO]  Epoch: 01 | Words: 6694528/28022587 | Lr: 0.00500 | Smooth loss: 21.18812\n",
      "[INFO]  Epoch: 01 | Words: 6725299/28022587 | Lr: 0.00500 | Smooth loss: 20.76859\n",
      "[INFO]  Epoch: 01 | Words: 6756588/28022587 | Lr: 0.00500 | Smooth loss: 21.43559\n",
      "[INFO]  Epoch: 01 | Words: 6787215/28022587 | Lr: 0.00500 | Smooth loss: 20.55330\n",
      "[INFO]  Epoch: 01 | Words: 6818144/28022587 | Lr: 0.00500 | Smooth loss: 20.41747\n",
      "[INFO]  Epoch: 01 | Words: 6848830/28022587 | Lr: 0.00500 | Smooth loss: 21.27601\n",
      "[INFO]  Epoch: 01 | Words: 6878553/28022587 | Lr: 0.00500 | Smooth loss: 21.03939\n",
      "[INFO]  Epoch: 01 | Words: 6908905/28022587 | Lr: 0.00500 | Smooth loss: 20.95220\n",
      "[INFO]  Epoch: 01 | Words: 6938842/28022587 | Lr: 0.00500 | Smooth loss: 20.43129\n",
      "[INFO]  Epoch: 01 | Words: 6969012/28022587 | Lr: 0.00500 | Smooth loss: 20.51501\n",
      "[INFO]  Epoch: 01 | Words: 6999137/28022587 | Lr: 0.00500 | Smooth loss: 21.65713\n",
      "[INFO]  Epoch: 01 | Words: 7029163/28022587 | Lr: 0.00500 | Smooth loss: 20.33572\n",
      "[INFO]  Epoch: 01 | Words: 7059573/28022587 | Lr: 0.00500 | Smooth loss: 20.22640\n",
      "[INFO]  Epoch: 01 | Words: 7089530/28022587 | Lr: 0.00500 | Smooth loss: 19.93516\n",
      "[INFO]  Epoch: 01 | Words: 7119709/28022587 | Lr: 0.00500 | Smooth loss: 19.93502\n",
      "[INFO]  Epoch: 01 | Words: 7149838/28022587 | Lr: 0.00500 | Smooth loss: 20.46507\n",
      "[INFO]  Epoch: 01 | Words: 7179868/28022587 | Lr: 0.00500 | Smooth loss: 19.82887\n",
      "[INFO]  Epoch: 01 | Words: 7210314/28022587 | Lr: 0.00500 | Smooth loss: 20.43998\n",
      "[INFO]  Epoch: 01 | Words: 7241391/28022587 | Lr: 0.00500 | Smooth loss: 20.88674\n",
      "[INFO]  Epoch: 01 | Words: 7272198/28022587 | Lr: 0.00500 | Smooth loss: 20.54865\n",
      "[INFO]  Epoch: 01 | Words: 7301266/28022587 | Lr: 0.00500 | Smooth loss: 20.99809\n",
      "[INFO]  Epoch: 01 | Words: 7331484/28022587 | Lr: 0.00500 | Smooth loss: 21.26913\n",
      "[INFO]  Epoch: 01 | Words: 7362436/28022587 | Lr: 0.00500 | Smooth loss: 20.19813\n",
      "[INFO]  Epoch: 01 | Words: 7393652/28022587 | Lr: 0.00500 | Smooth loss: 21.19422\n",
      "[INFO]  Epoch: 01 | Words: 7423213/28022587 | Lr: 0.00500 | Smooth loss: 20.47549\n",
      "[INFO]  Epoch: 01 | Words: 7453245/28022587 | Lr: 0.00500 | Smooth loss: 20.16180\n",
      "[INFO]  Epoch: 01 | Words: 7484833/28022587 | Lr: 0.00500 | Smooth loss: 20.39926\n",
      "[INFO]  Epoch: 01 | Words: 7514126/28022587 | Lr: 0.00500 | Smooth loss: 19.82872\n",
      "[INFO]  Epoch: 01 | Words: 7545997/28022587 | Lr: 0.00500 | Smooth loss: 20.58481\n",
      "[INFO]  Epoch: 01 | Words: 7576198/28022587 | Lr: 0.00500 | Smooth loss: 20.14928\n",
      "[INFO]  Epoch: 01 | Words: 7606503/28022587 | Lr: 0.00500 | Smooth loss: 20.01096\n",
      "[INFO]  Epoch: 01 | Words: 7636813/28022587 | Lr: 0.00500 | Smooth loss: 20.21294\n",
      "[INFO]  Epoch: 01 | Words: 7667063/28022587 | Lr: 0.00500 | Smooth loss: 19.81114\n",
      "[INFO]  Epoch: 01 | Words: 7697634/28022587 | Lr: 0.00500 | Smooth loss: 20.88351\n",
      "[INFO]  Epoch: 01 | Words: 7727239/28022587 | Lr: 0.00500 | Smooth loss: 20.10679\n",
      "[INFO]  Epoch: 01 | Words: 7757205/28022587 | Lr: 0.00500 | Smooth loss: 20.23507\n",
      "[INFO]  Epoch: 01 | Words: 7788357/28022587 | Lr: 0.00500 | Smooth loss: 20.43918\n",
      "[INFO]  Epoch: 01 | Words: 7819087/28022587 | Lr: 0.00500 | Smooth loss: 20.88023\n",
      "[INFO]  Epoch: 01 | Words: 7849750/28022587 | Lr: 0.00500 | Smooth loss: 20.07795\n",
      "[INFO]  Epoch: 01 | Words: 7879276/28022587 | Lr: 0.00500 | Smooth loss: 20.38542\n",
      "[INFO]  Epoch: 01 | Words: 7909410/28022587 | Lr: 0.00500 | Smooth loss: 20.09621\n",
      "[INFO]  Epoch: 01 | Words: 7939542/28022587 | Lr: 0.00500 | Smooth loss: 20.41195\n",
      "[INFO]  Epoch: 01 | Words: 7971000/28022587 | Lr: 0.00500 | Smooth loss: 20.45605\n",
      "[INFO]  Epoch: 01 | Words: 8001898/28022587 | Lr: 0.00500 | Smooth loss: 20.13311\n",
      "[INFO]  Epoch: 01 | Words: 8031704/28022587 | Lr: 0.00500 | Smooth loss: 20.14438\n",
      "[INFO]  Epoch: 01 | Words: 8062778/28022587 | Lr: 0.00500 | Smooth loss: 20.88646\n",
      "[INFO]  Epoch: 01 | Words: 8094290/28022587 | Lr: 0.00500 | Smooth loss: 20.11228\n",
      "[INFO]  Epoch: 01 | Words: 8124431/28022587 | Lr: 0.00500 | Smooth loss: 20.13002\n",
      "[INFO]  Epoch: 01 | Words: 8154818/28022587 | Lr: 0.00500 | Smooth loss: 19.95269\n",
      "[INFO]  Epoch: 01 | Words: 8184625/28022587 | Lr: 0.00500 | Smooth loss: 20.30244\n",
      "[INFO]  Epoch: 01 | Words: 8215072/28022587 | Lr: 0.00500 | Smooth loss: 20.23098\n",
      "[INFO]  Epoch: 01 | Words: 8245882/28022587 | Lr: 0.00500 | Smooth loss: 19.29015\n",
      "[INFO]  Epoch: 01 | Words: 8277424/28022587 | Lr: 0.00500 | Smooth loss: 20.40290\n",
      "[INFO]  Epoch: 01 | Words: 8307465/28022587 | Lr: 0.00500 | Smooth loss: 20.36485\n",
      "[INFO]  Epoch: 01 | Words: 8338658/28022587 | Lr: 0.00500 | Smooth loss: 20.43645\n",
      "[INFO]  Epoch: 01 | Words: 8368716/28022587 | Lr: 0.00500 | Smooth loss: 19.38991\n",
      "[INFO]  Epoch: 01 | Words: 8399859/28022587 | Lr: 0.00500 | Smooth loss: 19.83174\n",
      "[INFO]  Epoch: 01 | Words: 8429971/28022587 | Lr: 0.00500 | Smooth loss: 19.50586\n",
      "[INFO]  Epoch: 01 | Words: 8461287/28022587 | Lr: 0.00500 | Smooth loss: 19.50978\n",
      "[INFO]  Epoch: 01 | Words: 8493102/28022587 | Lr: 0.00500 | Smooth loss: 20.30971\n",
      "[INFO]  Epoch: 01 | Words: 8523236/28022587 | Lr: 0.00500 | Smooth loss: 20.46812\n",
      "[INFO]  Epoch: 01 | Words: 8554451/28022587 | Lr: 0.00500 | Smooth loss: 19.51195\n",
      "[INFO]  Epoch: 01 | Words: 8584845/28022587 | Lr: 0.00500 | Smooth loss: 19.71351\n",
      "[INFO]  Epoch: 01 | Words: 8615046/28022587 | Lr: 0.00500 | Smooth loss: 19.92690\n",
      "[INFO]  Epoch: 01 | Words: 8644329/28022587 | Lr: 0.00500 | Smooth loss: 19.42775\n",
      "[INFO]  Epoch: 01 | Words: 8673999/28022587 | Lr: 0.00500 | Smooth loss: 20.18498\n",
      "[INFO]  Epoch: 01 | Words: 8705237/28022587 | Lr: 0.00500 | Smooth loss: 20.24990\n",
      "[INFO]  Epoch: 01 | Words: 8736423/28022587 | Lr: 0.00500 | Smooth loss: 19.48513\n",
      "[INFO]  Epoch: 01 | Words: 8766496/28022587 | Lr: 0.00500 | Smooth loss: 19.89989\n",
      "[INFO]  Epoch: 01 | Words: 8795800/28022587 | Lr: 0.00500 | Smooth loss: 19.19563\n",
      "[INFO]  Epoch: 01 | Words: 8826377/28022587 | Lr: 0.00500 | Smooth loss: 20.09559\n",
      "[INFO]  Epoch: 01 | Words: 8856567/28022587 | Lr: 0.00500 | Smooth loss: 20.32219\n",
      "[INFO]  Epoch: 01 | Words: 8886164/28022587 | Lr: 0.00500 | Smooth loss: 19.52724\n",
      "[INFO]  Epoch: 01 | Words: 8916643/28022587 | Lr: 0.00500 | Smooth loss: 20.10271\n",
      "[INFO]  Epoch: 01 | Words: 8946313/28022587 | Lr: 0.00500 | Smooth loss: 19.95594\n",
      "[INFO]  Epoch: 01 | Words: 8976566/28022587 | Lr: 0.00500 | Smooth loss: 19.55796\n",
      "[INFO]  Epoch: 01 | Words: 9006824/28022587 | Lr: 0.00500 | Smooth loss: 20.37343\n",
      "[INFO]  Epoch: 01 | Words: 9036335/28022587 | Lr: 0.00500 | Smooth loss: 19.21067\n",
      "[INFO]  Epoch: 01 | Words: 9066132/28022587 | Lr: 0.00500 | Smooth loss: 19.71440\n",
      "[INFO]  Epoch: 01 | Words: 9096483/28022587 | Lr: 0.00500 | Smooth loss: 19.83920\n",
      "[INFO]  Epoch: 01 | Words: 9126343/28022587 | Lr: 0.00500 | Smooth loss: 20.26508\n",
      "[INFO]  Epoch: 01 | Words: 9156463/28022587 | Lr: 0.00500 | Smooth loss: 19.87793\n",
      "[INFO]  Epoch: 01 | Words: 9186279/28022587 | Lr: 0.00500 | Smooth loss: 19.83849\n",
      "[INFO]  Epoch: 01 | Words: 9216372/28022587 | Lr: 0.00500 | Smooth loss: 19.74360\n",
      "[INFO]  Epoch: 01 | Words: 9245959/28022587 | Lr: 0.00500 | Smooth loss: 19.89985\n",
      "[INFO]  Epoch: 01 | Words: 9275812/28022587 | Lr: 0.00500 | Smooth loss: 19.68697\n",
      "[INFO]  Epoch: 01 | Words: 9306873/28022587 | Lr: 0.00500 | Smooth loss: 19.67226\n",
      "[INFO]  Epoch: 01 | Words: 9338403/28022587 | Lr: 0.00500 | Smooth loss: 20.44426\n",
      "[INFO]  Epoch: 01 | Words: 9369053/28022587 | Lr: 0.00500 | Smooth loss: 19.35312\n",
      "[INFO]  Epoch: 01 | Words: 9399323/28022587 | Lr: 0.00500 | Smooth loss: 18.93618\n",
      "[INFO]  Epoch: 01 | Words: 9429302/28022587 | Lr: 0.00500 | Smooth loss: 19.29226\n",
      "[INFO]  Epoch: 01 | Words: 9459485/28022587 | Lr: 0.00500 | Smooth loss: 19.75573\n",
      "[INFO]  Epoch: 01 | Words: 9490077/28022587 | Lr: 0.00500 | Smooth loss: 19.91698\n",
      "[INFO]  Epoch: 01 | Words: 9521379/28022587 | Lr: 0.00500 | Smooth loss: 19.65529\n",
      "[INFO]  Epoch: 01 | Words: 9552006/28022587 | Lr: 0.00500 | Smooth loss: 19.87330\n",
      "[INFO]  Epoch: 01 | Words: 9582067/28022587 | Lr: 0.00500 | Smooth loss: 19.06079\n",
      "[INFO]  Epoch: 01 | Words: 9611916/28022587 | Lr: 0.00500 | Smooth loss: 20.14884\n",
      "[INFO]  Epoch: 01 | Words: 9643559/28022587 | Lr: 0.00500 | Smooth loss: 19.66369\n",
      "[INFO]  Epoch: 01 | Words: 9673913/28022587 | Lr: 0.00500 | Smooth loss: 19.47708\n",
      "[INFO]  Epoch: 01 | Words: 9705168/28022587 | Lr: 0.00500 | Smooth loss: 19.54222\n",
      "[INFO]  Epoch: 01 | Words: 9735613/28022587 | Lr: 0.00500 | Smooth loss: 19.38667\n",
      "[INFO]  Epoch: 01 | Words: 9766251/28022587 | Lr: 0.00500 | Smooth loss: 19.22302\n",
      "[INFO]  Epoch: 01 | Words: 9796844/28022587 | Lr: 0.00500 | Smooth loss: 19.47705\n",
      "[INFO]  Epoch: 01 | Words: 9827528/28022587 | Lr: 0.00500 | Smooth loss: 18.73455\n",
      "[INFO]  Epoch: 01 | Words: 9858253/28022587 | Lr: 0.00500 | Smooth loss: 19.64150\n",
      "[INFO]  Epoch: 01 | Words: 9887806/28022587 | Lr: 0.00500 | Smooth loss: 18.59531\n",
      "[INFO]  Epoch: 01 | Words: 9918401/28022587 | Lr: 0.00500 | Smooth loss: 19.95074\n",
      "[INFO]  Epoch: 01 | Words: 9947830/28022587 | Lr: 0.00500 | Smooth loss: 19.12119\n",
      "[INFO]  Epoch: 01 | Words: 9978603/28022587 | Lr: 0.00500 | Smooth loss: 19.79105\n",
      "[INFO]  Epoch: 01 | Words: 10009537/28022587 | Lr: 0.00500 | Smooth loss: 19.12353\n",
      "[INFO]  Epoch: 01 | Words: 10040300/28022587 | Lr: 0.00500 | Smooth loss: 19.28590\n",
      "[INFO]  Epoch: 01 | Words: 10070975/28022587 | Lr: 0.00500 | Smooth loss: 19.16938\n",
      "[INFO]  Epoch: 01 | Words: 10102137/28022587 | Lr: 0.00500 | Smooth loss: 19.24911\n",
      "[INFO]  Epoch: 01 | Words: 10132855/28022587 | Lr: 0.00500 | Smooth loss: 19.55994\n",
      "[INFO]  Epoch: 01 | Words: 10163325/28022587 | Lr: 0.00500 | Smooth loss: 19.65306\n",
      "[INFO]  Epoch: 01 | Words: 10194275/28022587 | Lr: 0.00500 | Smooth loss: 19.31913\n",
      "[INFO]  Epoch: 01 | Words: 10224268/28022587 | Lr: 0.00500 | Smooth loss: 19.01541\n",
      "[INFO]  Epoch: 01 | Words: 10254126/28022587 | Lr: 0.00500 | Smooth loss: 18.68127\n",
      "[INFO]  Epoch: 01 | Words: 10284567/28022587 | Lr: 0.00500 | Smooth loss: 19.00364\n",
      "[INFO]  Epoch: 01 | Words: 10313437/28022587 | Lr: 0.00500 | Smooth loss: 19.08698\n",
      "[INFO]  Epoch: 01 | Words: 10344291/28022587 | Lr: 0.00500 | Smooth loss: 19.85303\n",
      "[INFO]  Epoch: 01 | Words: 10374661/28022587 | Lr: 0.00500 | Smooth loss: 19.29084\n",
      "[INFO]  Epoch: 01 | Words: 10404515/28022587 | Lr: 0.00500 | Smooth loss: 19.27936\n",
      "[INFO]  Epoch: 01 | Words: 10434710/28022587 | Lr: 0.00500 | Smooth loss: 19.82666\n",
      "[INFO]  Epoch: 01 | Words: 10464350/28022587 | Lr: 0.00500 | Smooth loss: 19.06962\n",
      "[INFO]  Epoch: 01 | Words: 10494796/28022587 | Lr: 0.00500 | Smooth loss: 19.03606\n",
      "[INFO]  Epoch: 01 | Words: 10525121/28022587 | Lr: 0.00500 | Smooth loss: 18.67990\n",
      "[INFO]  Epoch: 01 | Words: 10555977/28022587 | Lr: 0.00500 | Smooth loss: 19.01127\n",
      "[INFO]  Epoch: 01 | Words: 10586735/28022587 | Lr: 0.00500 | Smooth loss: 19.32219\n",
      "[INFO]  Epoch: 01 | Words: 10617454/28022587 | Lr: 0.00500 | Smooth loss: 19.61988\n",
      "[INFO]  Epoch: 01 | Words: 10647720/28022587 | Lr: 0.00500 | Smooth loss: 18.80456\n",
      "[INFO]  Epoch: 01 | Words: 10677067/28022587 | Lr: 0.00500 | Smooth loss: 19.28587\n",
      "[INFO]  Epoch: 01 | Words: 10707367/28022587 | Lr: 0.00500 | Smooth loss: 19.65758\n",
      "[INFO]  Epoch: 01 | Words: 10737002/28022587 | Lr: 0.00500 | Smooth loss: 18.89912\n",
      "[INFO]  Epoch: 01 | Words: 10767676/28022587 | Lr: 0.00500 | Smooth loss: 18.96292\n",
      "[INFO]  Epoch: 01 | Words: 10798828/28022587 | Lr: 0.00500 | Smooth loss: 19.00999\n",
      "[INFO]  Epoch: 01 | Words: 10828573/28022587 | Lr: 0.00500 | Smooth loss: 18.34942\n",
      "[INFO]  Epoch: 01 | Words: 10860169/28022587 | Lr: 0.00500 | Smooth loss: 18.89452\n",
      "[INFO]  Epoch: 01 | Words: 10891409/28022587 | Lr: 0.00500 | Smooth loss: 19.53194\n",
      "[INFO]  Epoch: 01 | Words: 10921424/28022587 | Lr: 0.00500 | Smooth loss: 18.43835\n",
      "[INFO]  Epoch: 01 | Words: 10952279/28022587 | Lr: 0.00500 | Smooth loss: 19.06910\n",
      "[INFO]  Epoch: 01 | Words: 10983086/28022587 | Lr: 0.00500 | Smooth loss: 18.34554\n",
      "[INFO]  Epoch: 01 | Words: 11013443/28022587 | Lr: 0.00500 | Smooth loss: 19.71515\n",
      "[INFO]  Epoch: 01 | Words: 11043556/28022587 | Lr: 0.00500 | Smooth loss: 18.65926\n",
      "[INFO]  Epoch: 01 | Words: 11073596/28022587 | Lr: 0.00500 | Smooth loss: 18.53042\n",
      "[INFO]  Epoch: 01 | Words: 11103797/28022587 | Lr: 0.00500 | Smooth loss: 18.81687\n",
      "[INFO]  Epoch: 01 | Words: 11134159/28022587 | Lr: 0.00500 | Smooth loss: 19.43015\n",
      "[INFO]  Epoch: 01 | Words: 11163377/28022587 | Lr: 0.00500 | Smooth loss: 18.85162\n",
      "[INFO]  Epoch: 01 | Words: 11193606/28022587 | Lr: 0.00500 | Smooth loss: 18.96342\n",
      "[INFO]  Epoch: 01 | Words: 11223552/28022587 | Lr: 0.00500 | Smooth loss: 18.65007\n",
      "[INFO]  Epoch: 01 | Words: 11254345/28022587 | Lr: 0.00500 | Smooth loss: 18.67854\n",
      "[INFO]  Epoch: 01 | Words: 11284719/28022587 | Lr: 0.00500 | Smooth loss: 19.25734\n",
      "[INFO]  Epoch: 01 | Words: 11314068/28022587 | Lr: 0.00500 | Smooth loss: 18.19099\n",
      "[INFO]  Epoch: 01 | Words: 11345535/28022587 | Lr: 0.00500 | Smooth loss: 18.59827\n",
      "[INFO]  Epoch: 01 | Words: 11376484/28022587 | Lr: 0.00500 | Smooth loss: 19.10759\n",
      "[INFO]  Epoch: 01 | Words: 11407745/28022587 | Lr: 0.00500 | Smooth loss: 19.03304\n",
      "[INFO]  Epoch: 01 | Words: 11436535/28022587 | Lr: 0.00500 | Smooth loss: 19.11439\n",
      "[INFO]  Epoch: 01 | Words: 11466096/28022587 | Lr: 0.00500 | Smooth loss: 18.49149\n",
      "[INFO]  Epoch: 01 | Words: 11497366/28022587 | Lr: 0.00500 | Smooth loss: 19.67091\n",
      "[INFO]  Epoch: 01 | Words: 11527469/28022587 | Lr: 0.00500 | Smooth loss: 18.59050\n",
      "[INFO]  Epoch: 01 | Words: 11557260/28022587 | Lr: 0.00500 | Smooth loss: 18.69167\n",
      "[INFO]  Epoch: 01 | Words: 11588275/28022587 | Lr: 0.00500 | Smooth loss: 19.06133\n",
      "[INFO]  Epoch: 01 | Words: 11619535/28022587 | Lr: 0.00500 | Smooth loss: 19.05315\n",
      "[INFO]  Epoch: 01 | Words: 11650518/28022587 | Lr: 0.00500 | Smooth loss: 19.03675\n",
      "[INFO]  Epoch: 01 | Words: 11680182/28022587 | Lr: 0.00500 | Smooth loss: 18.52253\n",
      "[INFO]  Epoch: 01 | Words: 11710797/28022587 | Lr: 0.00500 | Smooth loss: 18.45115\n",
      "[INFO]  Epoch: 01 | Words: 11741371/28022587 | Lr: 0.00500 | Smooth loss: 18.92834\n",
      "[INFO]  Epoch: 01 | Words: 11772064/28022587 | Lr: 0.00500 | Smooth loss: 19.05987\n",
      "[INFO]  Epoch: 01 | Words: 11802385/28022587 | Lr: 0.00500 | Smooth loss: 19.21374\n",
      "[INFO]  Epoch: 01 | Words: 11832460/28022587 | Lr: 0.00500 | Smooth loss: 18.63029\n",
      "[INFO]  Epoch: 01 | Words: 11863107/28022587 | Lr: 0.00500 | Smooth loss: 18.34932\n",
      "[INFO]  Epoch: 01 | Words: 11894220/28022587 | Lr: 0.00500 | Smooth loss: 18.63553\n",
      "[INFO]  Epoch: 01 | Words: 11924587/28022587 | Lr: 0.00500 | Smooth loss: 18.30573\n",
      "[INFO]  Epoch: 01 | Words: 11954300/28022587 | Lr: 0.00500 | Smooth loss: 18.82165\n",
      "[INFO]  Epoch: 01 | Words: 11983873/28022587 | Lr: 0.00500 | Smooth loss: 18.34011\n",
      "[INFO]  Epoch: 01 | Words: 12014881/28022587 | Lr: 0.00500 | Smooth loss: 18.98428\n",
      "[INFO]  Epoch: 01 | Words: 12045823/28022587 | Lr: 0.00500 | Smooth loss: 19.11835\n",
      "[INFO]  Epoch: 01 | Words: 12076628/28022587 | Lr: 0.00500 | Smooth loss: 19.06908\n",
      "[INFO]  Epoch: 01 | Words: 12107426/28022587 | Lr: 0.00500 | Smooth loss: 18.49873\n",
      "[INFO]  Epoch: 01 | Words: 12139537/28022587 | Lr: 0.00500 | Smooth loss: 19.11159\n",
      "[INFO]  Epoch: 01 | Words: 12169346/28022587 | Lr: 0.00500 | Smooth loss: 18.48251\n",
      "[INFO]  Epoch: 01 | Words: 12199195/28022587 | Lr: 0.00500 | Smooth loss: 18.84860\n",
      "[INFO]  Epoch: 01 | Words: 12231452/28022587 | Lr: 0.00500 | Smooth loss: 19.27780\n",
      "[INFO]  Epoch: 01 | Words: 12261978/28022587 | Lr: 0.00500 | Smooth loss: 18.64522\n",
      "[INFO]  Epoch: 01 | Words: 12293425/28022587 | Lr: 0.00500 | Smooth loss: 18.88632\n",
      "[INFO]  Epoch: 01 | Words: 12323997/28022587 | Lr: 0.00500 | Smooth loss: 18.55017\n",
      "[INFO]  Epoch: 01 | Words: 12354632/28022587 | Lr: 0.00500 | Smooth loss: 18.95671\n",
      "[INFO]  Epoch: 01 | Words: 12384070/28022587 | Lr: 0.00500 | Smooth loss: 18.66055\n",
      "[INFO]  Epoch: 01 | Words: 12413370/28022587 | Lr: 0.00500 | Smooth loss: 18.30835\n",
      "[INFO]  Epoch: 01 | Words: 12443695/28022587 | Lr: 0.00500 | Smooth loss: 18.76013\n",
      "[INFO]  Epoch: 01 | Words: 12474330/28022587 | Lr: 0.00500 | Smooth loss: 18.82654\n",
      "[INFO]  Epoch: 01 | Words: 12503987/28022587 | Lr: 0.00500 | Smooth loss: 17.99592\n",
      "[INFO]  Epoch: 01 | Words: 12535498/28022587 | Lr: 0.00500 | Smooth loss: 18.07518\n",
      "[INFO]  Epoch: 01 | Words: 12565877/28022587 | Lr: 0.00500 | Smooth loss: 18.34649\n",
      "[INFO]  Epoch: 01 | Words: 12596436/28022587 | Lr: 0.00500 | Smooth loss: 18.43740\n",
      "[INFO]  Epoch: 01 | Words: 12626316/28022587 | Lr: 0.00500 | Smooth loss: 18.88282\n",
      "[INFO]  Epoch: 01 | Words: 12656297/28022587 | Lr: 0.00500 | Smooth loss: 18.81936\n",
      "[INFO]  Epoch: 01 | Words: 12686823/28022587 | Lr: 0.00500 | Smooth loss: 18.51322\n",
      "[INFO]  Epoch: 01 | Words: 12715760/28022587 | Lr: 0.00500 | Smooth loss: 17.60275\n",
      "[INFO]  Epoch: 01 | Words: 12744849/28022587 | Lr: 0.00500 | Smooth loss: 18.25890\n",
      "[INFO]  Epoch: 01 | Words: 12774651/28022587 | Lr: 0.00500 | Smooth loss: 18.24065\n",
      "[INFO]  Epoch: 01 | Words: 12804091/28022587 | Lr: 0.00500 | Smooth loss: 18.22497\n",
      "[INFO]  Epoch: 01 | Words: 12833850/28022587 | Lr: 0.00500 | Smooth loss: 18.57265\n",
      "[INFO]  Epoch: 01 | Words: 12864011/28022587 | Lr: 0.00500 | Smooth loss: 18.74138\n",
      "[INFO]  Epoch: 01 | Words: 12893712/28022587 | Lr: 0.00500 | Smooth loss: 18.55439\n",
      "[INFO]  Epoch: 01 | Words: 12924276/28022587 | Lr: 0.00500 | Smooth loss: 18.21226\n",
      "[INFO]  Epoch: 01 | Words: 12954463/28022587 | Lr: 0.00500 | Smooth loss: 17.99393\n",
      "[INFO]  Epoch: 01 | Words: 12985864/28022587 | Lr: 0.00500 | Smooth loss: 18.75544\n",
      "[INFO]  Epoch: 01 | Words: 13014362/28022587 | Lr: 0.00500 | Smooth loss: 17.99093\n",
      "[INFO]  Epoch: 01 | Words: 13045131/28022587 | Lr: 0.00500 | Smooth loss: 18.46294\n",
      "[INFO]  Epoch: 01 | Words: 13075958/28022587 | Lr: 0.00500 | Smooth loss: 18.52919\n",
      "[INFO]  Epoch: 01 | Words: 13106842/28022587 | Lr: 0.00500 | Smooth loss: 18.65010\n",
      "[INFO]  Epoch: 01 | Words: 13136794/28022587 | Lr: 0.00500 | Smooth loss: 17.94415\n",
      "[INFO]  Epoch: 01 | Words: 13166903/28022587 | Lr: 0.00500 | Smooth loss: 18.05081\n",
      "[INFO]  Epoch: 01 | Words: 13197470/28022587 | Lr: 0.00500 | Smooth loss: 18.69505\n",
      "[INFO]  Epoch: 01 | Words: 13227780/28022587 | Lr: 0.00500 | Smooth loss: 18.19156\n",
      "[INFO]  Epoch: 01 | Words: 13257977/28022587 | Lr: 0.00500 | Smooth loss: 18.26830\n",
      "[INFO]  Epoch: 01 | Words: 13288031/28022587 | Lr: 0.00500 | Smooth loss: 18.41347\n",
      "[INFO]  Epoch: 01 | Words: 13317838/28022587 | Lr: 0.00500 | Smooth loss: 18.15039\n",
      "[INFO]  Epoch: 01 | Words: 13347684/28022587 | Lr: 0.00500 | Smooth loss: 18.39368\n",
      "[INFO]  Epoch: 01 | Words: 13377507/28022587 | Lr: 0.00500 | Smooth loss: 17.80357\n",
      "[INFO]  Epoch: 01 | Words: 13407083/28022587 | Lr: 0.00500 | Smooth loss: 18.60020\n",
      "[INFO]  Epoch: 01 | Words: 13437409/28022587 | Lr: 0.00500 | Smooth loss: 17.38161\n",
      "[INFO]  Epoch: 01 | Words: 13468998/28022587 | Lr: 0.00500 | Smooth loss: 17.89842\n",
      "[INFO]  Epoch: 01 | Words: 13498417/28022587 | Lr: 0.00500 | Smooth loss: 18.09304\n",
      "[INFO]  Epoch: 01 | Words: 13528795/28022587 | Lr: 0.00500 | Smooth loss: 18.20917\n",
      "[INFO]  Epoch: 01 | Words: 13559041/28022587 | Lr: 0.00500 | Smooth loss: 18.21753\n",
      "[INFO]  Epoch: 01 | Words: 13589370/28022587 | Lr: 0.00500 | Smooth loss: 18.20207\n",
      "[INFO]  Epoch: 01 | Words: 13618707/28022587 | Lr: 0.00500 | Smooth loss: 18.12052\n",
      "[INFO]  Epoch: 01 | Words: 13649117/28022587 | Lr: 0.00500 | Smooth loss: 18.26445\n",
      "[INFO]  Epoch: 01 | Words: 13679039/28022587 | Lr: 0.00500 | Smooth loss: 17.92253\n",
      "[INFO]  Epoch: 01 | Words: 13710766/28022587 | Lr: 0.00500 | Smooth loss: 18.31978\n",
      "[INFO]  Epoch: 01 | Words: 13741607/28022587 | Lr: 0.00500 | Smooth loss: 18.51198\n",
      "[INFO]  Epoch: 01 | Words: 13771892/28022587 | Lr: 0.00500 | Smooth loss: 17.78793\n",
      "[INFO]  Epoch: 01 | Words: 13801910/28022587 | Lr: 0.00500 | Smooth loss: 18.30773\n",
      "[INFO]  Epoch: 01 | Words: 13832212/28022587 | Lr: 0.00500 | Smooth loss: 18.46062\n",
      "[INFO]  Epoch: 01 | Words: 13862563/28022587 | Lr: 0.00500 | Smooth loss: 17.83104\n",
      "[INFO]  Epoch: 01 | Words: 13893065/28022587 | Lr: 0.00500 | Smooth loss: 17.98647\n",
      "[INFO]  Epoch: 01 | Words: 13923391/28022587 | Lr: 0.00500 | Smooth loss: 18.36363\n",
      "[INFO]  Epoch: 01 | Words: 13952808/28022587 | Lr: 0.00500 | Smooth loss: 18.22278\n",
      "[INFO]  Epoch: 01 | Words: 13982802/28022587 | Lr: 0.00500 | Smooth loss: 18.05417\n",
      "[INFO]  Epoch: 01 | Words: 14012723/28022587 | Lr: 0.00500 | Smooth loss: 18.96384\n",
      "[INFO]  Epoch: 01 | Words: 14042168/28022587 | Lr: 0.00500 | Smooth loss: 16.81538\n",
      "[INFO]  Epoch: 01 | Words: 14072913/28022587 | Lr: 0.00500 | Smooth loss: 18.28574\n",
      "[INFO]  Epoch: 01 | Words: 14102497/28022587 | Lr: 0.00500 | Smooth loss: 17.69481\n",
      "[INFO]  Epoch: 01 | Words: 14133186/28022587 | Lr: 0.00500 | Smooth loss: 18.32382\n",
      "[INFO]  Epoch: 01 | Words: 14163475/28022587 | Lr: 0.00500 | Smooth loss: 17.97760\n",
      "[INFO]  Epoch: 01 | Words: 14193452/28022587 | Lr: 0.00500 | Smooth loss: 17.86351\n",
      "[INFO]  Epoch: 01 | Words: 14223235/28022587 | Lr: 0.00500 | Smooth loss: 17.59100\n",
      "[INFO]  Epoch: 01 | Words: 14251981/28022587 | Lr: 0.00500 | Smooth loss: 17.58448\n",
      "[INFO]  Epoch: 01 | Words: 14281337/28022587 | Lr: 0.00500 | Smooth loss: 18.25491\n",
      "[INFO]  Epoch: 01 | Words: 14311908/28022587 | Lr: 0.00500 | Smooth loss: 18.11701\n",
      "[INFO]  Epoch: 01 | Words: 14342230/28022587 | Lr: 0.00500 | Smooth loss: 17.99535\n",
      "[INFO]  Epoch: 01 | Words: 14373924/28022587 | Lr: 0.00500 | Smooth loss: 18.01411\n",
      "[INFO]  Epoch: 01 | Words: 14404923/28022587 | Lr: 0.00500 | Smooth loss: 18.05818\n",
      "[INFO]  Epoch: 01 | Words: 14435632/28022587 | Lr: 0.00500 | Smooth loss: 17.73768\n",
      "[INFO]  Epoch: 01 | Words: 14466952/28022587 | Lr: 0.00500 | Smooth loss: 17.84392\n",
      "[INFO]  Epoch: 01 | Words: 14497207/28022587 | Lr: 0.00500 | Smooth loss: 18.22434\n",
      "[INFO]  Epoch: 01 | Words: 14526673/28022587 | Lr: 0.00500 | Smooth loss: 17.81167\n",
      "[INFO]  Epoch: 01 | Words: 14557576/28022587 | Lr: 0.00500 | Smooth loss: 18.26024\n",
      "[INFO]  Epoch: 01 | Words: 14586691/28022587 | Lr: 0.00500 | Smooth loss: 17.62360\n",
      "[INFO]  Epoch: 01 | Words: 14616871/28022587 | Lr: 0.00500 | Smooth loss: 18.12597\n",
      "[INFO]  Epoch: 01 | Words: 14647311/28022587 | Lr: 0.00500 | Smooth loss: 18.13711\n",
      "[INFO]  Epoch: 01 | Words: 14676583/28022587 | Lr: 0.00500 | Smooth loss: 18.25699\n",
      "[INFO]  Epoch: 01 | Words: 14707150/28022587 | Lr: 0.00500 | Smooth loss: 18.13532\n",
      "[INFO]  Epoch: 01 | Words: 14737614/28022587 | Lr: 0.00500 | Smooth loss: 18.07353\n",
      "[INFO]  Epoch: 01 | Words: 14767733/28022587 | Lr: 0.00500 | Smooth loss: 17.52183\n",
      "[INFO]  Epoch: 01 | Words: 14797273/28022587 | Lr: 0.00500 | Smooth loss: 17.44884\n",
      "[INFO]  Epoch: 01 | Words: 14827290/28022587 | Lr: 0.00500 | Smooth loss: 17.36645\n",
      "[INFO]  Epoch: 01 | Words: 14857957/28022587 | Lr: 0.00500 | Smooth loss: 18.45168\n",
      "[INFO]  Epoch: 01 | Words: 14888053/28022587 | Lr: 0.00500 | Smooth loss: 17.95140\n",
      "[INFO]  Epoch: 01 | Words: 14919323/28022587 | Lr: 0.00500 | Smooth loss: 18.50670\n",
      "[INFO]  Epoch: 01 | Words: 14949716/28022587 | Lr: 0.00500 | Smooth loss: 17.62831\n",
      "[INFO]  Epoch: 01 | Words: 14979834/28022587 | Lr: 0.00500 | Smooth loss: 17.74046\n",
      "[INFO]  Epoch: 01 | Words: 15010778/28022587 | Lr: 0.00500 | Smooth loss: 17.62563\n",
      "[INFO]  Epoch: 01 | Words: 15040295/28022587 | Lr: 0.00500 | Smooth loss: 17.55095\n",
      "[INFO]  Epoch: 01 | Words: 15069828/28022587 | Lr: 0.00500 | Smooth loss: 17.65633\n",
      "[INFO]  Epoch: 01 | Words: 15101195/28022587 | Lr: 0.00500 | Smooth loss: 18.51177\n",
      "[INFO]  Epoch: 01 | Words: 15131507/28022587 | Lr: 0.00500 | Smooth loss: 17.87155\n",
      "[INFO]  Epoch: 01 | Words: 15161592/28022587 | Lr: 0.00500 | Smooth loss: 17.83507\n",
      "[INFO]  Epoch: 01 | Words: 15191106/28022587 | Lr: 0.00500 | Smooth loss: 17.53814\n",
      "[INFO]  Epoch: 01 | Words: 15222133/28022587 | Lr: 0.00500 | Smooth loss: 18.12087\n",
      "[INFO]  Epoch: 01 | Words: 15251950/28022587 | Lr: 0.00500 | Smooth loss: 17.90347\n",
      "[INFO]  Epoch: 01 | Words: 15281947/28022587 | Lr: 0.00500 | Smooth loss: 17.99986\n",
      "[INFO]  Epoch: 01 | Words: 15313135/28022587 | Lr: 0.00500 | Smooth loss: 18.14869\n",
      "[INFO]  Epoch: 01 | Words: 15342338/28022587 | Lr: 0.00500 | Smooth loss: 17.21671\n",
      "[INFO]  Epoch: 01 | Words: 15372941/28022587 | Lr: 0.00500 | Smooth loss: 17.88482\n",
      "[INFO]  Epoch: 01 | Words: 15403389/28022587 | Lr: 0.00500 | Smooth loss: 18.05851\n",
      "[INFO]  Epoch: 01 | Words: 15433202/28022587 | Lr: 0.00500 | Smooth loss: 17.34101\n",
      "[INFO]  Epoch: 01 | Words: 15462525/28022587 | Lr: 0.00500 | Smooth loss: 16.87799\n",
      "[INFO]  Epoch: 01 | Words: 15492993/28022587 | Lr: 0.00500 | Smooth loss: 17.94447\n",
      "[INFO]  Epoch: 01 | Words: 15521490/28022587 | Lr: 0.00500 | Smooth loss: 17.30797\n",
      "[INFO]  Epoch: 01 | Words: 15552017/28022587 | Lr: 0.00500 | Smooth loss: 18.08244\n",
      "[INFO]  Epoch: 01 | Words: 15582371/28022587 | Lr: 0.00500 | Smooth loss: 18.20406\n",
      "[INFO]  Epoch: 01 | Words: 15612930/28022587 | Lr: 0.00500 | Smooth loss: 17.68876\n",
      "[INFO]  Epoch: 01 | Words: 15642792/28022587 | Lr: 0.00500 | Smooth loss: 17.75266\n",
      "[INFO]  Epoch: 01 | Words: 15673304/28022587 | Lr: 0.00500 | Smooth loss: 17.93316\n",
      "[INFO]  Epoch: 01 | Words: 15705291/28022587 | Lr: 0.00500 | Smooth loss: 18.15801\n",
      "[INFO]  Epoch: 01 | Words: 15735421/28022587 | Lr: 0.00500 | Smooth loss: 17.91332\n",
      "[INFO]  Epoch: 01 | Words: 15766345/28022587 | Lr: 0.00500 | Smooth loss: 17.63932\n",
      "[INFO]  Epoch: 01 | Words: 15797169/28022587 | Lr: 0.00500 | Smooth loss: 17.93896\n",
      "[INFO]  Epoch: 01 | Words: 15826526/28022587 | Lr: 0.00500 | Smooth loss: 17.06895\n",
      "[INFO]  Epoch: 01 | Words: 15856781/28022587 | Lr: 0.00500 | Smooth loss: 18.15702\n",
      "[INFO]  Epoch: 01 | Words: 15886307/28022587 | Lr: 0.00500 | Smooth loss: 17.85602\n",
      "[INFO]  Epoch: 01 | Words: 15916963/28022587 | Lr: 0.00500 | Smooth loss: 18.14560\n",
      "[INFO]  Epoch: 01 | Words: 15946568/28022587 | Lr: 0.00500 | Smooth loss: 17.07482\n",
      "[INFO]  Epoch: 01 | Words: 15976782/28022587 | Lr: 0.00500 | Smooth loss: 17.27072\n",
      "[INFO]  Epoch: 01 | Words: 16007326/28022587 | Lr: 0.00500 | Smooth loss: 17.46442\n",
      "[INFO]  Epoch: 01 | Words: 16037562/28022587 | Lr: 0.00500 | Smooth loss: 18.24934\n",
      "[INFO]  Epoch: 01 | Words: 16068189/28022587 | Lr: 0.00500 | Smooth loss: 17.57319\n",
      "[INFO]  Epoch: 01 | Words: 16098392/28022587 | Lr: 0.00500 | Smooth loss: 17.82982\n",
      "[INFO]  Epoch: 01 | Words: 16128090/28022587 | Lr: 0.00500 | Smooth loss: 17.64006\n",
      "[INFO]  Epoch: 01 | Words: 16159275/28022587 | Lr: 0.00500 | Smooth loss: 17.76124\n",
      "[INFO]  Epoch: 01 | Words: 16189521/28022587 | Lr: 0.00500 | Smooth loss: 17.59541\n",
      "[INFO]  Epoch: 01 | Words: 16219573/28022587 | Lr: 0.00500 | Smooth loss: 17.39383\n",
      "[INFO]  Epoch: 01 | Words: 16250333/28022587 | Lr: 0.00500 | Smooth loss: 17.90990\n",
      "[INFO]  Epoch: 01 | Words: 16280841/28022587 | Lr: 0.00500 | Smooth loss: 17.78980\n",
      "[INFO]  Epoch: 01 | Words: 16312462/28022587 | Lr: 0.00500 | Smooth loss: 17.64678\n",
      "[INFO]  Epoch: 01 | Words: 16343851/28022587 | Lr: 0.00500 | Smooth loss: 17.25281\n",
      "[INFO]  Epoch: 01 | Words: 16374590/28022587 | Lr: 0.00500 | Smooth loss: 17.88856\n",
      "[INFO]  Epoch: 01 | Words: 16406030/28022587 | Lr: 0.00500 | Smooth loss: 17.56143\n",
      "[INFO]  Epoch: 01 | Words: 16434889/28022587 | Lr: 0.00500 | Smooth loss: 17.12044\n",
      "[INFO]  Epoch: 01 | Words: 16465803/28022587 | Lr: 0.00500 | Smooth loss: 17.71490\n",
      "[INFO]  Epoch: 01 | Words: 16495409/28022587 | Lr: 0.00500 | Smooth loss: 17.32849\n",
      "[INFO]  Epoch: 01 | Words: 16525587/28022587 | Lr: 0.00500 | Smooth loss: 17.96181\n",
      "[INFO]  Epoch: 01 | Words: 16555023/28022587 | Lr: 0.00500 | Smooth loss: 17.46416\n",
      "[INFO]  Epoch: 01 | Words: 16585253/28022587 | Lr: 0.00500 | Smooth loss: 17.53832\n",
      "[INFO]  Epoch: 01 | Words: 16616217/28022587 | Lr: 0.00500 | Smooth loss: 17.93201\n",
      "[INFO]  Epoch: 01 | Words: 16645882/28022587 | Lr: 0.00500 | Smooth loss: 17.13852\n",
      "[INFO]  Epoch: 01 | Words: 16676543/28022587 | Lr: 0.00500 | Smooth loss: 17.97373\n",
      "[INFO]  Epoch: 01 | Words: 16705664/28022587 | Lr: 0.00500 | Smooth loss: 17.39607\n",
      "[INFO]  Epoch: 01 | Words: 16736602/28022587 | Lr: 0.00500 | Smooth loss: 17.59060\n",
      "[INFO]  Epoch: 01 | Words: 16768173/28022587 | Lr: 0.00500 | Smooth loss: 17.88880\n",
      "[INFO]  Epoch: 01 | Words: 16798911/28022587 | Lr: 0.00500 | Smooth loss: 18.01829\n",
      "[INFO]  Epoch: 01 | Words: 16828546/28022587 | Lr: 0.00500 | Smooth loss: 17.18702\n",
      "[INFO]  Epoch: 01 | Words: 16859584/28022587 | Lr: 0.00500 | Smooth loss: 17.65394\n",
      "[INFO]  Epoch: 01 | Words: 16891271/28022587 | Lr: 0.00500 | Smooth loss: 17.90214\n",
      "[INFO]  Epoch: 01 | Words: 16922210/28022587 | Lr: 0.00500 | Smooth loss: 18.25982\n",
      "[INFO]  Epoch: 01 | Words: 16952396/28022587 | Lr: 0.00500 | Smooth loss: 18.04621\n",
      "[INFO]  Epoch: 01 | Words: 16982850/28022587 | Lr: 0.00500 | Smooth loss: 17.91214\n",
      "[INFO]  Epoch: 01 | Words: 17012190/28022587 | Lr: 0.00500 | Smooth loss: 17.42958\n",
      "[INFO]  Epoch: 01 | Words: 17042448/28022587 | Lr: 0.00500 | Smooth loss: 17.53500\n",
      "[INFO]  Epoch: 01 | Words: 17071959/28022587 | Lr: 0.00500 | Smooth loss: 17.03127\n",
      "[INFO]  Epoch: 01 | Words: 17103129/28022587 | Lr: 0.00500 | Smooth loss: 17.17000\n",
      "[INFO]  Epoch: 01 | Words: 17133325/28022587 | Lr: 0.00500 | Smooth loss: 17.75168\n",
      "[INFO]  Epoch: 01 | Words: 17164354/28022587 | Lr: 0.00500 | Smooth loss: 17.55457\n",
      "[INFO]  Epoch: 01 | Words: 17194486/28022587 | Lr: 0.00500 | Smooth loss: 17.35809\n",
      "[INFO]  Epoch: 01 | Words: 17224075/28022587 | Lr: 0.00500 | Smooth loss: 17.65329\n",
      "[INFO]  Epoch: 01 | Words: 17255119/28022587 | Lr: 0.00500 | Smooth loss: 18.10067\n",
      "[INFO]  Epoch: 01 | Words: 17285667/28022587 | Lr: 0.00500 | Smooth loss: 17.88251\n",
      "[INFO]  Epoch: 01 | Words: 17315216/28022587 | Lr: 0.00500 | Smooth loss: 17.70767\n",
      "[INFO]  Epoch: 01 | Words: 17345550/28022587 | Lr: 0.00500 | Smooth loss: 17.56281\n",
      "[INFO]  Epoch: 01 | Words: 17375140/28022587 | Lr: 0.00500 | Smooth loss: 16.87747\n",
      "[INFO]  Epoch: 01 | Words: 17405678/28022587 | Lr: 0.00500 | Smooth loss: 17.76143\n",
      "[INFO]  Epoch: 01 | Words: 17435764/28022587 | Lr: 0.00500 | Smooth loss: 17.65173\n",
      "[INFO]  Epoch: 01 | Words: 17465868/28022587 | Lr: 0.00500 | Smooth loss: 17.44880\n",
      "[INFO]  Epoch: 01 | Words: 17497231/28022587 | Lr: 0.00500 | Smooth loss: 17.83558\n",
      "[INFO]  Epoch: 01 | Words: 17527295/28022587 | Lr: 0.00500 | Smooth loss: 17.48251\n",
      "[INFO]  Epoch: 01 | Words: 17557248/28022587 | Lr: 0.00500 | Smooth loss: 17.37303\n",
      "[INFO]  Epoch: 01 | Words: 17587055/28022587 | Lr: 0.00500 | Smooth loss: 17.73291\n",
      "[INFO]  Epoch: 01 | Words: 17617466/28022587 | Lr: 0.00500 | Smooth loss: 17.61130\n",
      "[INFO]  Epoch: 01 | Words: 17646938/28022587 | Lr: 0.00500 | Smooth loss: 17.06240\n",
      "[INFO]  Epoch: 01 | Words: 17677814/28022587 | Lr: 0.00500 | Smooth loss: 17.41508\n",
      "[INFO]  Epoch: 01 | Words: 17707554/28022587 | Lr: 0.00500 | Smooth loss: 17.45878\n",
      "[INFO]  Epoch: 01 | Words: 17738071/28022587 | Lr: 0.00500 | Smooth loss: 17.94580\n",
      "[INFO]  Epoch: 01 | Words: 17767677/28022587 | Lr: 0.00500 | Smooth loss: 17.40594\n",
      "[INFO]  Epoch: 01 | Words: 17797466/28022587 | Lr: 0.00500 | Smooth loss: 17.28811\n",
      "[INFO]  Epoch: 01 | Words: 17827725/28022587 | Lr: 0.00500 | Smooth loss: 17.88599\n",
      "[INFO]  Epoch: 01 | Words: 17858015/28022587 | Lr: 0.00500 | Smooth loss: 17.18356\n",
      "[INFO]  Epoch: 01 | Words: 17886096/28022587 | Lr: 0.00500 | Smooth loss: 16.57583\n",
      "[INFO]  Epoch: 01 | Words: 17917508/28022587 | Lr: 0.00500 | Smooth loss: 17.44199\n",
      "[INFO]  Epoch: 01 | Words: 17947616/28022587 | Lr: 0.00500 | Smooth loss: 17.13112\n",
      "[INFO]  Epoch: 01 | Words: 17978001/28022587 | Lr: 0.00500 | Smooth loss: 17.79173\n",
      "[INFO]  Epoch: 01 | Words: 18008084/28022587 | Lr: 0.00500 | Smooth loss: 17.28938\n",
      "[INFO]  Epoch: 01 | Words: 18038341/28022587 | Lr: 0.00500 | Smooth loss: 17.53480\n",
      "[INFO]  Epoch: 01 | Words: 18069823/28022587 | Lr: 0.00500 | Smooth loss: 17.68773\n",
      "[INFO]  Epoch: 01 | Words: 18099501/28022587 | Lr: 0.00500 | Smooth loss: 16.97534\n",
      "[INFO]  Epoch: 01 | Words: 18130253/28022587 | Lr: 0.00500 | Smooth loss: 17.37826\n",
      "[INFO]  Epoch: 01 | Words: 18160712/28022587 | Lr: 0.00500 | Smooth loss: 17.37001\n",
      "[INFO]  Epoch: 01 | Words: 18191356/28022587 | Lr: 0.00500 | Smooth loss: 17.60208\n",
      "[INFO]  Epoch: 01 | Words: 18222076/28022587 | Lr: 0.00500 | Smooth loss: 17.13898\n",
      "[INFO]  Epoch: 01 | Words: 18252090/28022587 | Lr: 0.00500 | Smooth loss: 16.82448\n",
      "[INFO]  Epoch: 01 | Words: 18281859/28022587 | Lr: 0.00500 | Smooth loss: 16.96587\n",
      "[INFO]  Epoch: 01 | Words: 18312129/28022587 | Lr: 0.00500 | Smooth loss: 16.74096\n",
      "[INFO]  Epoch: 01 | Words: 18341132/28022587 | Lr: 0.00500 | Smooth loss: 17.22400\n",
      "[INFO]  Epoch: 01 | Words: 18372668/28022587 | Lr: 0.00500 | Smooth loss: 17.18122\n",
      "[INFO]  Epoch: 01 | Words: 18403390/28022587 | Lr: 0.00500 | Smooth loss: 17.37721\n",
      "[INFO]  Epoch: 01 | Words: 18432814/28022587 | Lr: 0.00500 | Smooth loss: 16.96381\n",
      "[INFO]  Epoch: 01 | Words: 18463528/28022587 | Lr: 0.00500 | Smooth loss: 17.09715\n",
      "[INFO]  Epoch: 01 | Words: 18494282/28022587 | Lr: 0.00500 | Smooth loss: 17.24251\n",
      "[INFO]  Epoch: 01 | Words: 18524673/28022587 | Lr: 0.00500 | Smooth loss: 17.53510\n",
      "[INFO]  Epoch: 01 | Words: 18555319/28022587 | Lr: 0.00500 | Smooth loss: 17.41035\n",
      "[INFO]  Epoch: 01 | Words: 18585231/28022587 | Lr: 0.00500 | Smooth loss: 17.65020\n",
      "[INFO]  Epoch: 01 | Words: 18615913/28022587 | Lr: 0.00500 | Smooth loss: 17.54571\n",
      "[INFO]  Epoch: 01 | Words: 18647082/28022587 | Lr: 0.00500 | Smooth loss: 17.24621\n",
      "[INFO]  Epoch: 01 | Words: 18677961/28022587 | Lr: 0.00500 | Smooth loss: 17.86657\n",
      "[INFO]  Epoch: 01 | Words: 18707589/28022587 | Lr: 0.00500 | Smooth loss: 17.14526\n",
      "[INFO]  Epoch: 01 | Words: 18737912/28022587 | Lr: 0.00500 | Smooth loss: 17.31249\n",
      "[INFO]  Epoch: 01 | Words: 18767504/28022587 | Lr: 0.00500 | Smooth loss: 16.92973\n",
      "[INFO]  Epoch: 01 | Words: 18797106/28022587 | Lr: 0.00500 | Smooth loss: 16.61460\n",
      "[INFO]  Epoch: 01 | Words: 18827325/28022587 | Lr: 0.00500 | Smooth loss: 17.18624\n",
      "[INFO]  Epoch: 01 | Words: 18857764/28022587 | Lr: 0.00500 | Smooth loss: 17.22726\n",
      "[INFO]  Epoch: 01 | Words: 18887878/28022587 | Lr: 0.00500 | Smooth loss: 16.99352\n",
      "[INFO]  Epoch: 01 | Words: 18917601/28022587 | Lr: 0.00500 | Smooth loss: 17.37223\n",
      "[INFO]  Epoch: 01 | Words: 18946850/28022587 | Lr: 0.00500 | Smooth loss: 17.52342\n",
      "[INFO]  Epoch: 01 | Words: 18977475/28022587 | Lr: 0.00500 | Smooth loss: 16.74542\n",
      "[INFO]  Epoch: 01 | Words: 19008711/28022587 | Lr: 0.00500 | Smooth loss: 17.09737\n",
      "[INFO]  Epoch: 01 | Words: 19039431/28022587 | Lr: 0.00500 | Smooth loss: 17.41296\n",
      "[INFO]  Epoch: 01 | Words: 19069134/28022587 | Lr: 0.00500 | Smooth loss: 16.96573\n",
      "[INFO]  Epoch: 01 | Words: 19099614/28022587 | Lr: 0.00500 | Smooth loss: 16.98286\n",
      "[INFO]  Epoch: 01 | Words: 19130669/28022587 | Lr: 0.00500 | Smooth loss: 17.43329\n",
      "[INFO]  Epoch: 01 | Words: 19160676/28022587 | Lr: 0.00500 | Smooth loss: 17.22676\n",
      "[INFO]  Epoch: 01 | Words: 19190335/28022587 | Lr: 0.00500 | Smooth loss: 17.23565\n",
      "[INFO]  Epoch: 01 | Words: 19220919/28022587 | Lr: 0.00500 | Smooth loss: 17.76186\n",
      "[INFO]  Epoch: 01 | Words: 19251009/28022587 | Lr: 0.00500 | Smooth loss: 17.34005\n",
      "[INFO]  Epoch: 01 | Words: 19281964/28022587 | Lr: 0.00500 | Smooth loss: 17.34264\n",
      "[INFO]  Epoch: 01 | Words: 19312192/28022587 | Lr: 0.00500 | Smooth loss: 16.92844\n",
      "[INFO]  Epoch: 01 | Words: 19342183/28022587 | Lr: 0.00500 | Smooth loss: 16.81725\n",
      "[INFO]  Epoch: 01 | Words: 19372243/28022587 | Lr: 0.00500 | Smooth loss: 16.86890\n",
      "[INFO]  Epoch: 01 | Words: 19402799/28022587 | Lr: 0.00500 | Smooth loss: 17.29046\n",
      "[INFO]  Epoch: 01 | Words: 19432950/28022587 | Lr: 0.00500 | Smooth loss: 17.35065\n",
      "[INFO]  Epoch: 01 | Words: 19464146/28022587 | Lr: 0.00500 | Smooth loss: 17.34041\n",
      "[INFO]  Epoch: 01 | Words: 19493097/28022587 | Lr: 0.00500 | Smooth loss: 17.14471\n",
      "[INFO]  Epoch: 01 | Words: 19523332/28022587 | Lr: 0.00500 | Smooth loss: 17.33840\n",
      "[INFO]  Epoch: 01 | Words: 19552478/28022587 | Lr: 0.00500 | Smooth loss: 16.18673\n",
      "[INFO]  Epoch: 01 | Words: 19582722/28022587 | Lr: 0.00500 | Smooth loss: 17.30881\n",
      "[INFO]  Epoch: 01 | Words: 19612334/28022587 | Lr: 0.00500 | Smooth loss: 16.86681\n",
      "[INFO]  Epoch: 01 | Words: 19642143/28022587 | Lr: 0.00500 | Smooth loss: 16.72799\n",
      "[INFO]  Epoch: 01 | Words: 19673133/28022587 | Lr: 0.00500 | Smooth loss: 17.70724\n",
      "[INFO]  Epoch: 01 | Words: 19703227/28022587 | Lr: 0.00500 | Smooth loss: 17.04801\n",
      "[INFO]  Epoch: 01 | Words: 19732472/28022587 | Lr: 0.00500 | Smooth loss: 17.36474\n",
      "[INFO]  Epoch: 01 | Words: 19763010/28022587 | Lr: 0.00500 | Smooth loss: 17.19945\n",
      "[INFO]  Epoch: 01 | Words: 19792606/28022587 | Lr: 0.00500 | Smooth loss: 16.92475\n",
      "[INFO]  Epoch: 01 | Words: 19822400/28022587 | Lr: 0.00500 | Smooth loss: 16.91351\n",
      "[INFO]  Epoch: 01 | Words: 19853625/28022587 | Lr: 0.00500 | Smooth loss: 17.27948\n",
      "[INFO]  Epoch: 01 | Words: 19882835/28022587 | Lr: 0.00500 | Smooth loss: 16.43095\n",
      "[INFO]  Epoch: 01 | Words: 19914129/28022587 | Lr: 0.00500 | Smooth loss: 17.17495\n",
      "[INFO]  Epoch: 01 | Words: 19943903/28022587 | Lr: 0.00500 | Smooth loss: 17.48316\n",
      "[INFO]  Epoch: 01 | Words: 19973880/28022587 | Lr: 0.00500 | Smooth loss: 17.36974\n",
      "[INFO]  Epoch: 01 | Words: 20004786/28022587 | Lr: 0.00500 | Smooth loss: 17.37339\n",
      "[INFO]  Epoch: 01 | Words: 20035449/28022587 | Lr: 0.00500 | Smooth loss: 17.14145\n",
      "[INFO]  Epoch: 01 | Words: 20065367/28022587 | Lr: 0.00500 | Smooth loss: 17.16698\n",
      "[INFO]  Epoch: 01 | Words: 20095681/28022587 | Lr: 0.00500 | Smooth loss: 17.22531\n",
      "[INFO]  Epoch: 01 | Words: 20124832/28022587 | Lr: 0.00500 | Smooth loss: 17.29594\n",
      "[INFO]  Epoch: 01 | Words: 20154922/28022587 | Lr: 0.00500 | Smooth loss: 17.20953\n",
      "[INFO]  Epoch: 01 | Words: 20184451/28022587 | Lr: 0.00500 | Smooth loss: 17.15518\n",
      "[INFO]  Epoch: 01 | Words: 20214832/28022587 | Lr: 0.00500 | Smooth loss: 17.37977\n",
      "[INFO]  Epoch: 01 | Words: 20244992/28022587 | Lr: 0.00500 | Smooth loss: 17.27851\n",
      "[INFO]  Epoch: 01 | Words: 20274956/28022587 | Lr: 0.00500 | Smooth loss: 16.70851\n",
      "[INFO]  Epoch: 01 | Words: 20305504/28022587 | Lr: 0.00500 | Smooth loss: 16.93191\n",
      "[INFO]  Epoch: 01 | Words: 20336111/28022587 | Lr: 0.00500 | Smooth loss: 16.95397\n",
      "[INFO]  Epoch: 01 | Words: 20366508/28022587 | Lr: 0.00500 | Smooth loss: 16.88622\n",
      "[INFO]  Epoch: 01 | Words: 20395135/28022587 | Lr: 0.00500 | Smooth loss: 16.54163\n",
      "[INFO]  Epoch: 01 | Words: 20424450/28022587 | Lr: 0.00500 | Smooth loss: 16.86732\n",
      "[INFO]  Epoch: 01 | Words: 20454856/28022587 | Lr: 0.00500 | Smooth loss: 17.10783\n",
      "[INFO]  Epoch: 01 | Words: 20485432/28022587 | Lr: 0.00500 | Smooth loss: 17.41738\n",
      "[INFO]  Epoch: 01 | Words: 20516064/28022587 | Lr: 0.00500 | Smooth loss: 17.24514\n",
      "[INFO]  Epoch: 01 | Words: 20546414/28022587 | Lr: 0.00500 | Smooth loss: 16.97098\n",
      "[INFO]  Epoch: 01 | Words: 20576373/28022587 | Lr: 0.00500 | Smooth loss: 17.12606\n",
      "[INFO]  Epoch: 01 | Words: 20606405/28022587 | Lr: 0.00500 | Smooth loss: 16.94724\n",
      "[INFO]  Epoch: 01 | Words: 20637712/28022587 | Lr: 0.00500 | Smooth loss: 17.04799\n",
      "[INFO]  Epoch: 01 | Words: 20668805/28022587 | Lr: 0.00500 | Smooth loss: 16.73823\n",
      "[INFO]  Epoch: 01 | Words: 20700269/28022587 | Lr: 0.00500 | Smooth loss: 17.44241\n",
      "[INFO]  Epoch: 01 | Words: 20730480/28022587 | Lr: 0.00500 | Smooth loss: 17.27720\n",
      "[INFO]  Epoch: 01 | Words: 20761968/28022587 | Lr: 0.00500 | Smooth loss: 17.54486\n",
      "[INFO]  Epoch: 01 | Words: 20793090/28022587 | Lr: 0.00500 | Smooth loss: 17.36171\n",
      "[INFO]  Epoch: 01 | Words: 20824323/28022587 | Lr: 0.00500 | Smooth loss: 16.86973\n",
      "[INFO]  Epoch: 01 | Words: 20854361/28022587 | Lr: 0.00500 | Smooth loss: 16.68909\n",
      "[INFO]  Epoch: 01 | Words: 20883293/28022587 | Lr: 0.00500 | Smooth loss: 16.88124\n",
      "[INFO]  Epoch: 01 | Words: 20913528/28022587 | Lr: 0.00500 | Smooth loss: 17.02870\n",
      "[INFO]  Epoch: 01 | Words: 20944653/28022587 | Lr: 0.00500 | Smooth loss: 16.92413\n",
      "[INFO]  Epoch: 01 | Words: 20975487/28022587 | Lr: 0.00500 | Smooth loss: 17.46547\n",
      "[INFO]  Epoch: 01 | Words: 21005298/28022587 | Lr: 0.00500 | Smooth loss: 16.96384\n",
      "[INFO]  Epoch: 01 | Words: 21035751/28022587 | Lr: 0.00500 | Smooth loss: 16.87946\n",
      "[INFO]  Epoch: 01 | Words: 21066292/28022587 | Lr: 0.00500 | Smooth loss: 17.27495\n",
      "[INFO]  Epoch: 01 | Words: 21096894/28022587 | Lr: 0.00500 | Smooth loss: 17.42733\n",
      "[INFO]  Epoch: 01 | Words: 21126305/28022587 | Lr: 0.00500 | Smooth loss: 16.92434\n",
      "[INFO]  Epoch: 01 | Words: 21157209/28022587 | Lr: 0.00500 | Smooth loss: 17.17031\n",
      "[INFO]  Epoch: 01 | Words: 21187472/28022587 | Lr: 0.00500 | Smooth loss: 16.99275\n",
      "[INFO]  Epoch: 01 | Words: 21217250/28022587 | Lr: 0.00500 | Smooth loss: 16.78628\n",
      "[INFO]  Epoch: 01 | Words: 21247596/28022587 | Lr: 0.00500 | Smooth loss: 17.13092\n",
      "[INFO]  Epoch: 01 | Words: 21278913/28022587 | Lr: 0.00500 | Smooth loss: 17.27536\n",
      "[INFO]  Epoch: 01 | Words: 21309445/28022587 | Lr: 0.00500 | Smooth loss: 16.79971\n",
      "[INFO]  Epoch: 01 | Words: 21338969/28022587 | Lr: 0.00500 | Smooth loss: 16.80786\n",
      "[INFO]  Epoch: 01 | Words: 21369124/28022587 | Lr: 0.00500 | Smooth loss: 16.64935\n",
      "[INFO]  Epoch: 01 | Words: 21399302/28022587 | Lr: 0.00500 | Smooth loss: 17.18141\n",
      "[INFO]  Epoch: 01 | Words: 21429912/28022587 | Lr: 0.00500 | Smooth loss: 17.03549\n",
      "[INFO]  Epoch: 01 | Words: 21460252/28022587 | Lr: 0.00500 | Smooth loss: 17.12997\n",
      "[INFO]  Epoch: 01 | Words: 21490781/28022587 | Lr: 0.00500 | Smooth loss: 17.05936\n",
      "[INFO]  Epoch: 01 | Words: 21520438/28022587 | Lr: 0.00500 | Smooth loss: 16.98053\n",
      "[INFO]  Epoch: 01 | Words: 21551805/28022587 | Lr: 0.00500 | Smooth loss: 17.09623\n",
      "[INFO]  Epoch: 01 | Words: 21581770/28022587 | Lr: 0.00500 | Smooth loss: 16.68531\n",
      "[INFO]  Epoch: 01 | Words: 21612566/28022587 | Lr: 0.00500 | Smooth loss: 17.02771\n",
      "[INFO]  Epoch: 01 | Words: 21642194/28022587 | Lr: 0.00500 | Smooth loss: 16.38662\n",
      "[INFO]  Epoch: 01 | Words: 21670140/28022587 | Lr: 0.00500 | Smooth loss: 16.60924\n",
      "[INFO]  Epoch: 01 | Words: 21700886/28022587 | Lr: 0.00500 | Smooth loss: 17.27661\n",
      "[INFO]  Epoch: 01 | Words: 21731384/28022587 | Lr: 0.00500 | Smooth loss: 16.84693\n",
      "[INFO]  Epoch: 01 | Words: 21761228/28022587 | Lr: 0.00500 | Smooth loss: 17.39490\n",
      "[INFO]  Epoch: 01 | Words: 21791335/28022587 | Lr: 0.00500 | Smooth loss: 17.30436\n",
      "[INFO]  Epoch: 01 | Words: 21822090/28022587 | Lr: 0.00500 | Smooth loss: 17.20580\n",
      "[INFO]  Epoch: 01 | Words: 21851757/28022587 | Lr: 0.00500 | Smooth loss: 16.56619\n",
      "[INFO]  Epoch: 01 | Words: 21882140/28022587 | Lr: 0.00500 | Smooth loss: 17.22877\n",
      "[INFO]  Epoch: 01 | Words: 21911820/28022587 | Lr: 0.00500 | Smooth loss: 16.96408\n",
      "[INFO]  Epoch: 01 | Words: 21940229/28022587 | Lr: 0.00500 | Smooth loss: 16.41609\n",
      "[INFO]  Epoch: 01 | Words: 21971194/28022587 | Lr: 0.00500 | Smooth loss: 17.48804\n",
      "[INFO]  Epoch: 01 | Words: 22001104/28022587 | Lr: 0.00500 | Smooth loss: 16.67954\n",
      "[INFO]  Epoch: 01 | Words: 22031223/28022587 | Lr: 0.00500 | Smooth loss: 17.24258\n",
      "[INFO]  Epoch: 01 | Words: 22062262/28022587 | Lr: 0.00500 | Smooth loss: 17.13039\n",
      "[INFO]  Epoch: 01 | Words: 22091363/28022587 | Lr: 0.00500 | Smooth loss: 17.12128\n",
      "[INFO]  Epoch: 01 | Words: 22122054/28022587 | Lr: 0.00500 | Smooth loss: 17.16756\n",
      "[INFO]  Epoch: 01 | Words: 22151497/28022587 | Lr: 0.00500 | Smooth loss: 17.07767\n",
      "[INFO]  Epoch: 01 | Words: 22180843/28022587 | Lr: 0.00500 | Smooth loss: 17.02835\n",
      "[INFO]  Epoch: 01 | Words: 22211814/28022587 | Lr: 0.00500 | Smooth loss: 16.89724\n",
      "[INFO]  Epoch: 01 | Words: 22241741/28022587 | Lr: 0.00500 | Smooth loss: 16.80004\n",
      "[INFO]  Epoch: 01 | Words: 22272094/28022587 | Lr: 0.00500 | Smooth loss: 17.08650\n",
      "[INFO]  Epoch: 01 | Words: 22302946/28022587 | Lr: 0.00500 | Smooth loss: 17.28832\n",
      "[INFO]  Epoch: 01 | Words: 22333177/28022587 | Lr: 0.00500 | Smooth loss: 16.81648\n",
      "[INFO]  Epoch: 01 | Words: 22364091/28022587 | Lr: 0.00500 | Smooth loss: 16.83460\n",
      "[INFO]  Epoch: 01 | Words: 22394627/28022587 | Lr: 0.00500 | Smooth loss: 17.30316\n",
      "[INFO]  Epoch: 01 | Words: 22425211/28022587 | Lr: 0.00500 | Smooth loss: 17.37403\n",
      "[INFO]  Epoch: 01 | Words: 22454958/28022587 | Lr: 0.00500 | Smooth loss: 16.68486\n",
      "[INFO]  Epoch: 01 | Words: 22485392/28022587 | Lr: 0.00500 | Smooth loss: 17.10162\n",
      "[INFO]  Epoch: 01 | Words: 22516033/28022587 | Lr: 0.00500 | Smooth loss: 16.94798\n",
      "[INFO]  Epoch: 01 | Words: 22546067/28022587 | Lr: 0.00500 | Smooth loss: 16.64665\n",
      "[INFO]  Epoch: 01 | Words: 22577238/28022587 | Lr: 0.00500 | Smooth loss: 17.09070\n",
      "[INFO]  Epoch: 01 | Words: 22606986/28022587 | Lr: 0.00500 | Smooth loss: 16.56660\n",
      "[INFO]  Epoch: 01 | Words: 22637990/28022587 | Lr: 0.00500 | Smooth loss: 16.92374\n",
      "[INFO]  Epoch: 01 | Words: 22668361/28022587 | Lr: 0.00500 | Smooth loss: 17.33775\n",
      "[INFO]  Epoch: 01 | Words: 22699408/28022587 | Lr: 0.00500 | Smooth loss: 16.98900\n",
      "[INFO]  Epoch: 01 | Words: 22730250/28022587 | Lr: 0.00500 | Smooth loss: 16.97167\n",
      "[INFO]  Epoch: 01 | Words: 22760642/28022587 | Lr: 0.00500 | Smooth loss: 16.71581\n",
      "[INFO]  Epoch: 01 | Words: 22791581/28022587 | Lr: 0.00500 | Smooth loss: 17.02296\n",
      "[INFO]  Epoch: 01 | Words: 22821406/28022587 | Lr: 0.00500 | Smooth loss: 16.57683\n",
      "[INFO]  Epoch: 01 | Words: 22852561/28022587 | Lr: 0.00500 | Smooth loss: 17.07221\n",
      "[INFO]  Epoch: 01 | Words: 22884596/28022587 | Lr: 0.00500 | Smooth loss: 17.15558\n",
      "[INFO]  Epoch: 01 | Words: 22915679/28022587 | Lr: 0.00500 | Smooth loss: 16.77339\n",
      "[INFO]  Epoch: 01 | Words: 22947301/28022587 | Lr: 0.00500 | Smooth loss: 17.14378\n",
      "[INFO]  Epoch: 01 | Words: 22977306/28022587 | Lr: 0.00500 | Smooth loss: 16.33492\n",
      "[INFO]  Epoch: 01 | Words: 23007977/28022587 | Lr: 0.00500 | Smooth loss: 16.93922\n",
      "[INFO]  Epoch: 01 | Words: 23038783/28022587 | Lr: 0.00500 | Smooth loss: 17.05561\n",
      "[INFO]  Epoch: 01 | Words: 23069316/28022587 | Lr: 0.00500 | Smooth loss: 16.69992\n",
      "[INFO]  Epoch: 01 | Words: 23100340/28022587 | Lr: 0.00500 | Smooth loss: 17.04067\n",
      "[INFO]  Epoch: 01 | Words: 23130590/28022587 | Lr: 0.00500 | Smooth loss: 17.31695\n",
      "[INFO]  Epoch: 01 | Words: 23161337/28022587 | Lr: 0.00500 | Smooth loss: 16.93668\n",
      "[INFO]  Epoch: 01 | Words: 23191401/28022587 | Lr: 0.00500 | Smooth loss: 16.66600\n",
      "[INFO]  Epoch: 01 | Words: 23222131/28022587 | Lr: 0.00500 | Smooth loss: 16.77975\n",
      "[INFO]  Epoch: 01 | Words: 23252241/28022587 | Lr: 0.00500 | Smooth loss: 16.71829\n",
      "[INFO]  Epoch: 01 | Words: 23282102/28022587 | Lr: 0.00500 | Smooth loss: 16.32522\n",
      "[INFO]  Epoch: 01 | Words: 23311888/28022587 | Lr: 0.00500 | Smooth loss: 16.96026\n",
      "[INFO]  Epoch: 01 | Words: 23341923/28022587 | Lr: 0.00500 | Smooth loss: 16.68521\n",
      "[INFO]  Epoch: 01 | Words: 23371474/28022587 | Lr: 0.00500 | Smooth loss: 16.52851\n",
      "[INFO]  Epoch: 01 | Words: 23401596/28022587 | Lr: 0.00500 | Smooth loss: 16.91576\n",
      "[INFO]  Epoch: 01 | Words: 23431703/28022587 | Lr: 0.00500 | Smooth loss: 17.05547\n",
      "[INFO]  Epoch: 01 | Words: 23460993/28022587 | Lr: 0.00500 | Smooth loss: 16.74617\n",
      "[INFO]  Epoch: 01 | Words: 23490730/28022587 | Lr: 0.00500 | Smooth loss: 16.85976\n",
      "[INFO]  Epoch: 01 | Words: 23520852/28022587 | Lr: 0.00500 | Smooth loss: 16.66144\n",
      "[INFO]  Epoch: 01 | Words: 23551095/28022587 | Lr: 0.00500 | Smooth loss: 16.78263\n",
      "[INFO]  Epoch: 01 | Words: 23582275/28022587 | Lr: 0.00500 | Smooth loss: 17.41572\n",
      "[INFO]  Epoch: 01 | Words: 23612591/28022587 | Lr: 0.00500 | Smooth loss: 16.98938\n",
      "[INFO]  Epoch: 01 | Words: 23643114/28022587 | Lr: 0.00500 | Smooth loss: 17.34740\n",
      "[INFO]  Epoch: 01 | Words: 23672866/28022587 | Lr: 0.00500 | Smooth loss: 16.53370\n",
      "[INFO]  Epoch: 01 | Words: 23702675/28022587 | Lr: 0.00500 | Smooth loss: 17.21717\n",
      "[INFO]  Epoch: 01 | Words: 23733299/28022587 | Lr: 0.00500 | Smooth loss: 17.20126\n",
      "[INFO]  Epoch: 01 | Words: 23763120/28022587 | Lr: 0.00500 | Smooth loss: 16.71886\n",
      "[INFO]  Epoch: 01 | Words: 23793863/28022587 | Lr: 0.00500 | Smooth loss: 17.19738\n",
      "[INFO]  Epoch: 01 | Words: 23824991/28022587 | Lr: 0.00500 | Smooth loss: 16.92720\n",
      "[INFO]  Epoch: 01 | Words: 23855445/28022587 | Lr: 0.00500 | Smooth loss: 16.82524\n",
      "[INFO]  Epoch: 01 | Words: 23885497/28022587 | Lr: 0.00500 | Smooth loss: 16.57987\n",
      "[INFO]  Epoch: 01 | Words: 23916061/28022587 | Lr: 0.00500 | Smooth loss: 17.31891\n",
      "[INFO]  Epoch: 01 | Words: 23947164/28022587 | Lr: 0.00500 | Smooth loss: 17.10767\n",
      "[INFO]  Epoch: 01 | Words: 23978500/28022587 | Lr: 0.00500 | Smooth loss: 16.86312\n",
      "[INFO]  Epoch: 01 | Words: 24007758/28022587 | Lr: 0.00500 | Smooth loss: 16.12341\n",
      "[INFO]  Epoch: 01 | Words: 24038520/28022587 | Lr: 0.00500 | Smooth loss: 16.59762\n",
      "[INFO]  Epoch: 01 | Words: 24069317/28022587 | Lr: 0.00500 | Smooth loss: 17.03662\n",
      "[INFO]  Epoch: 01 | Words: 24100136/28022587 | Lr: 0.00500 | Smooth loss: 16.96654\n",
      "[INFO]  Epoch: 01 | Words: 24131008/28022587 | Lr: 0.00500 | Smooth loss: 17.07120\n",
      "[INFO]  Epoch: 01 | Words: 24160508/28022587 | Lr: 0.00500 | Smooth loss: 16.52795\n",
      "[INFO]  Epoch: 01 | Words: 24190604/28022587 | Lr: 0.00500 | Smooth loss: 16.53321\n",
      "[INFO]  Epoch: 01 | Words: 24221872/28022587 | Lr: 0.00500 | Smooth loss: 17.06556\n",
      "[INFO]  Epoch: 01 | Words: 24249976/28022587 | Lr: 0.00500 | Smooth loss: 16.15272\n",
      "[INFO]  Epoch: 01 | Words: 24281473/28022587 | Lr: 0.00500 | Smooth loss: 16.85933\n",
      "[INFO]  Epoch: 01 | Words: 24312283/28022587 | Lr: 0.00500 | Smooth loss: 16.94540\n",
      "[INFO]  Epoch: 01 | Words: 24342815/28022587 | Lr: 0.00500 | Smooth loss: 16.89799\n",
      "[INFO]  Epoch: 01 | Words: 24372893/28022587 | Lr: 0.00500 | Smooth loss: 16.70585\n",
      "[INFO]  Epoch: 01 | Words: 24404235/28022587 | Lr: 0.00500 | Smooth loss: 16.97181\n",
      "[INFO]  Epoch: 01 | Words: 24434149/28022587 | Lr: 0.00500 | Smooth loss: 16.39493\n",
      "[INFO]  Epoch: 01 | Words: 24465202/28022587 | Lr: 0.00500 | Smooth loss: 17.00560\n",
      "[INFO]  Epoch: 01 | Words: 24494917/28022587 | Lr: 0.00500 | Smooth loss: 16.30103\n",
      "[INFO]  Epoch: 01 | Words: 24525402/28022587 | Lr: 0.00500 | Smooth loss: 16.35287\n",
      "[INFO]  Epoch: 01 | Words: 24555386/28022587 | Lr: 0.00500 | Smooth loss: 16.87582\n",
      "[INFO]  Epoch: 01 | Words: 24586392/28022587 | Lr: 0.00500 | Smooth loss: 17.21914\n",
      "[INFO]  Epoch: 01 | Words: 24616662/28022587 | Lr: 0.00500 | Smooth loss: 16.62815\n",
      "[INFO]  Epoch: 01 | Words: 24647005/28022587 | Lr: 0.00500 | Smooth loss: 16.59630\n",
      "[INFO]  Epoch: 01 | Words: 24676789/28022587 | Lr: 0.00500 | Smooth loss: 16.63174\n",
      "[INFO]  Epoch: 01 | Words: 24707566/28022587 | Lr: 0.00500 | Smooth loss: 16.51426\n",
      "[INFO]  Epoch: 01 | Words: 24738201/28022587 | Lr: 0.00500 | Smooth loss: 16.70764\n",
      "[INFO]  Epoch: 01 | Words: 24767579/28022587 | Lr: 0.00500 | Smooth loss: 16.54448\n",
      "[INFO]  Epoch: 01 | Words: 24797399/28022587 | Lr: 0.00500 | Smooth loss: 16.50347\n",
      "[INFO]  Epoch: 01 | Words: 24828127/28022587 | Lr: 0.00500 | Smooth loss: 16.83190\n",
      "[INFO]  Epoch: 01 | Words: 24857917/28022587 | Lr: 0.00500 | Smooth loss: 16.84681\n",
      "[INFO]  Epoch: 01 | Words: 24888366/28022587 | Lr: 0.00500 | Smooth loss: 17.05581\n",
      "[INFO]  Epoch: 01 | Words: 24918976/28022587 | Lr: 0.00500 | Smooth loss: 17.00404\n",
      "[INFO]  Epoch: 01 | Words: 24950015/28022587 | Lr: 0.00500 | Smooth loss: 17.42455\n",
      "[INFO]  Epoch: 01 | Words: 24979196/28022587 | Lr: 0.00500 | Smooth loss: 16.43468\n",
      "[INFO]  Epoch: 01 | Words: 25009443/28022587 | Lr: 0.00500 | Smooth loss: 16.52984\n",
      "[INFO]  Epoch: 01 | Words: 25040492/28022587 | Lr: 0.00500 | Smooth loss: 16.69751\n",
      "[INFO]  Epoch: 01 | Words: 25071067/28022587 | Lr: 0.00500 | Smooth loss: 17.20485\n",
      "[INFO]  Epoch: 01 | Words: 25101852/28022587 | Lr: 0.00500 | Smooth loss: 17.13648\n",
      "[INFO]  Epoch: 01 | Words: 25131800/28022587 | Lr: 0.00500 | Smooth loss: 16.93893\n",
      "[INFO]  Epoch: 01 | Words: 25161269/28022587 | Lr: 0.00500 | Smooth loss: 16.62107\n",
      "[INFO]  Epoch: 01 | Words: 25190698/28022587 | Lr: 0.00500 | Smooth loss: 16.45922\n",
      "[INFO]  Epoch: 01 | Words: 25221932/28022587 | Lr: 0.00500 | Smooth loss: 16.85949\n",
      "[INFO]  Epoch: 01 | Words: 25253048/28022587 | Lr: 0.00500 | Smooth loss: 16.85422\n",
      "[INFO]  Epoch: 01 | Words: 25283417/28022587 | Lr: 0.00500 | Smooth loss: 16.63928\n",
      "[INFO]  Epoch: 01 | Words: 25313398/28022587 | Lr: 0.00500 | Smooth loss: 16.59848\n",
      "[INFO]  Epoch: 01 | Words: 25343941/28022587 | Lr: 0.00500 | Smooth loss: 16.74113\n",
      "[INFO]  Epoch: 01 | Words: 25374592/28022587 | Lr: 0.00500 | Smooth loss: 16.56559\n",
      "[INFO]  Epoch: 01 | Words: 25405019/28022587 | Lr: 0.00500 | Smooth loss: 16.07849\n",
      "[INFO]  Epoch: 01 | Words: 25435704/28022587 | Lr: 0.00500 | Smooth loss: 17.14422\n",
      "[INFO]  Epoch: 01 | Words: 25466023/28022587 | Lr: 0.00500 | Smooth loss: 16.60666\n",
      "[INFO]  Epoch: 01 | Words: 25495647/28022587 | Lr: 0.00500 | Smooth loss: 16.28239\n",
      "[INFO]  Epoch: 01 | Words: 25527207/28022587 | Lr: 0.00500 | Smooth loss: 16.95227\n",
      "[INFO]  Epoch: 01 | Words: 25557998/28022587 | Lr: 0.00500 | Smooth loss: 16.82488\n",
      "[INFO]  Epoch: 01 | Words: 25588340/28022587 | Lr: 0.00500 | Smooth loss: 16.13972\n",
      "[INFO]  Epoch: 01 | Words: 25618826/28022587 | Lr: 0.00500 | Smooth loss: 16.38204\n",
      "[INFO]  Epoch: 01 | Words: 25649827/28022587 | Lr: 0.00500 | Smooth loss: 16.98996\n",
      "[INFO]  Epoch: 01 | Words: 25679656/28022587 | Lr: 0.00500 | Smooth loss: 16.87015\n",
      "[INFO]  Epoch: 01 | Words: 25710292/28022587 | Lr: 0.00500 | Smooth loss: 16.84275\n",
      "[INFO]  Epoch: 01 | Words: 25740379/28022587 | Lr: 0.00500 | Smooth loss: 16.84862\n",
      "[INFO]  Epoch: 01 | Words: 25770585/28022587 | Lr: 0.00500 | Smooth loss: 16.31282\n",
      "[INFO]  Epoch: 01 | Words: 25800590/28022587 | Lr: 0.00500 | Smooth loss: 16.83790\n",
      "[INFO]  Epoch: 01 | Words: 25830846/28022587 | Lr: 0.00500 | Smooth loss: 16.74854\n",
      "[INFO]  Epoch: 01 | Words: 25862008/28022587 | Lr: 0.00500 | Smooth loss: 17.25917\n",
      "[INFO]  Epoch: 01 | Words: 25891860/28022587 | Lr: 0.00500 | Smooth loss: 16.86107\n",
      "[INFO]  Epoch: 01 | Words: 25922512/28022587 | Lr: 0.00500 | Smooth loss: 16.79450\n",
      "[INFO]  Epoch: 01 | Words: 25952863/28022587 | Lr: 0.00500 | Smooth loss: 16.65049\n",
      "[INFO]  Epoch: 01 | Words: 25983578/28022587 | Lr: 0.00500 | Smooth loss: 16.60027\n",
      "[INFO]  Epoch: 01 | Words: 26013267/28022587 | Lr: 0.00500 | Smooth loss: 16.31200\n",
      "[INFO]  Epoch: 01 | Words: 26043578/28022587 | Lr: 0.00500 | Smooth loss: 16.30150\n",
      "[INFO]  Epoch: 01 | Words: 26073339/28022587 | Lr: 0.00500 | Smooth loss: 16.27737\n",
      "[INFO]  Epoch: 01 | Words: 26103199/28022587 | Lr: 0.00500 | Smooth loss: 16.30285\n",
      "[INFO]  Epoch: 01 | Words: 26133966/28022587 | Lr: 0.00500 | Smooth loss: 16.95955\n",
      "[INFO]  Epoch: 01 | Words: 26163074/28022587 | Lr: 0.00500 | Smooth loss: 16.56086\n",
      "[INFO]  Epoch: 01 | Words: 26192378/28022587 | Lr: 0.00500 | Smooth loss: 16.55355\n",
      "[INFO]  Epoch: 01 | Words: 26222200/28022587 | Lr: 0.00500 | Smooth loss: 15.64836\n",
      "[INFO]  Epoch: 01 | Words: 26252518/28022587 | Lr: 0.00500 | Smooth loss: 16.64503\n",
      "[INFO]  Epoch: 01 | Words: 26283114/28022587 | Lr: 0.00500 | Smooth loss: 16.42478\n",
      "[INFO]  Epoch: 01 | Words: 26313230/28022587 | Lr: 0.00500 | Smooth loss: 16.40179\n",
      "[INFO]  Epoch: 01 | Words: 26341912/28022587 | Lr: 0.00500 | Smooth loss: 16.60576\n",
      "[INFO]  Epoch: 01 | Words: 26371147/28022587 | Lr: 0.00500 | Smooth loss: 16.45298\n",
      "[INFO]  Epoch: 01 | Words: 26401417/28022587 | Lr: 0.00500 | Smooth loss: 16.44224\n",
      "[INFO]  Epoch: 01 | Words: 26432456/28022587 | Lr: 0.00500 | Smooth loss: 16.13210\n",
      "[INFO]  Epoch: 01 | Words: 26463021/28022587 | Lr: 0.00500 | Smooth loss: 16.62179\n",
      "[INFO]  Epoch: 01 | Words: 26493155/28022587 | Lr: 0.00500 | Smooth loss: 15.94761\n",
      "[INFO]  Epoch: 01 | Words: 26524486/28022587 | Lr: 0.00500 | Smooth loss: 16.87674\n",
      "[INFO]  Epoch: 01 | Words: 26556327/28022587 | Lr: 0.00500 | Smooth loss: 16.80209\n",
      "[INFO]  Epoch: 01 | Words: 26586210/28022587 | Lr: 0.00500 | Smooth loss: 16.65588\n",
      "[INFO]  Epoch: 01 | Words: 26616158/28022587 | Lr: 0.00500 | Smooth loss: 16.76922\n",
      "[INFO]  Epoch: 01 | Words: 26646003/28022587 | Lr: 0.00500 | Smooth loss: 16.71052\n",
      "[INFO]  Epoch: 01 | Words: 26676497/28022587 | Lr: 0.00500 | Smooth loss: 16.88860\n",
      "[INFO]  Epoch: 01 | Words: 26706505/28022587 | Lr: 0.00500 | Smooth loss: 16.55387\n",
      "[INFO]  Epoch: 01 | Words: 26735861/28022587 | Lr: 0.00500 | Smooth loss: 16.52309\n",
      "[INFO]  Epoch: 01 | Words: 26766597/28022587 | Lr: 0.00500 | Smooth loss: 17.01460\n",
      "[INFO]  Epoch: 01 | Words: 26797183/28022587 | Lr: 0.00500 | Smooth loss: 16.62980\n",
      "[INFO]  Epoch: 01 | Words: 26828226/28022587 | Lr: 0.00500 | Smooth loss: 16.72911\n",
      "[INFO]  Epoch: 01 | Words: 26858443/28022587 | Lr: 0.00500 | Smooth loss: 16.57785\n",
      "[INFO]  Epoch: 01 | Words: 26888879/28022587 | Lr: 0.00500 | Smooth loss: 16.07812\n",
      "[INFO]  Epoch: 01 | Words: 26918860/28022587 | Lr: 0.00500 | Smooth loss: 16.47070\n",
      "[INFO]  Epoch: 01 | Words: 26948947/28022587 | Lr: 0.00500 | Smooth loss: 16.18787\n",
      "[INFO]  Epoch: 01 | Words: 26978915/28022587 | Lr: 0.00500 | Smooth loss: 16.76760\n",
      "[INFO]  Epoch: 01 | Words: 27010009/28022587 | Lr: 0.00500 | Smooth loss: 16.98327\n",
      "[INFO]  Epoch: 01 | Words: 27040372/28022587 | Lr: 0.00500 | Smooth loss: 16.44257\n",
      "[INFO]  Epoch: 01 | Words: 27070820/28022587 | Lr: 0.00500 | Smooth loss: 16.65842\n",
      "[INFO]  Epoch: 01 | Words: 27101074/28022587 | Lr: 0.00500 | Smooth loss: 16.47270\n",
      "[INFO]  Epoch: 01 | Words: 27132222/28022587 | Lr: 0.00500 | Smooth loss: 16.72725\n",
      "[INFO]  Epoch: 01 | Words: 27161649/28022587 | Lr: 0.00500 | Smooth loss: 16.50584\n",
      "[INFO]  Epoch: 01 | Words: 27192137/28022587 | Lr: 0.00500 | Smooth loss: 16.81850\n",
      "[INFO]  Epoch: 01 | Words: 27222276/28022587 | Lr: 0.00500 | Smooth loss: 16.47404\n",
      "[INFO]  Epoch: 01 | Words: 27252718/28022587 | Lr: 0.00500 | Smooth loss: 16.55948\n",
      "[INFO]  Epoch: 01 | Words: 27281742/28022587 | Lr: 0.00500 | Smooth loss: 16.63848\n",
      "[INFO]  Epoch: 01 | Words: 27313089/28022587 | Lr: 0.00500 | Smooth loss: 16.71914\n",
      "[INFO]  Epoch: 01 | Words: 27342684/28022587 | Lr: 0.00500 | Smooth loss: 16.26698\n",
      "[INFO]  Epoch: 01 | Words: 27373136/28022587 | Lr: 0.00500 | Smooth loss: 16.64228\n",
      "[INFO]  Epoch: 01 | Words: 27404173/28022587 | Lr: 0.00500 | Smooth loss: 16.47789\n",
      "[INFO]  Epoch: 01 | Words: 27432537/28022587 | Lr: 0.00500 | Smooth loss: 15.89232\n",
      "[INFO]  Epoch: 01 | Words: 27463332/28022587 | Lr: 0.00500 | Smooth loss: 16.50701\n",
      "[INFO]  Epoch: 01 | Words: 27492932/28022587 | Lr: 0.00500 | Smooth loss: 16.33587\n",
      "[INFO]  Epoch: 01 | Words: 27522199/28022587 | Lr: 0.00500 | Smooth loss: 16.40368\n",
      "[INFO]  Epoch: 01 | Words: 27551966/28022587 | Lr: 0.00500 | Smooth loss: 16.18179\n",
      "[INFO]  Epoch: 01 | Words: 27581141/28022587 | Lr: 0.00500 | Smooth loss: 16.07045\n",
      "[INFO]  Epoch: 01 | Words: 27610538/28022587 | Lr: 0.00500 | Smooth loss: 16.27005\n",
      "[INFO]  Epoch: 01 | Words: 27641214/28022587 | Lr: 0.00500 | Smooth loss: 16.64010\n",
      "[INFO]  Epoch: 01 | Words: 27670705/28022587 | Lr: 0.00500 | Smooth loss: 16.18819\n",
      "[INFO]  Epoch: 01 | Words: 27700896/28022587 | Lr: 0.00500 | Smooth loss: 16.57184\n",
      "[INFO]  Epoch: 01 | Words: 27731524/28022587 | Lr: 0.00500 | Smooth loss: 16.79218\n",
      "[INFO]  Epoch: 01 | Words: 27761774/28022587 | Lr: 0.00500 | Smooth loss: 16.52116\n",
      "[INFO]  Epoch: 01 | Words: 27793059/28022587 | Lr: 0.00500 | Smooth loss: 16.91827\n",
      "[INFO]  Epoch: 01 | Words: 27822120/28022587 | Lr: 0.00500 | Smooth loss: 16.48979\n",
      "[INFO]  Epoch: 01 | Words: 27852668/28022587 | Lr: 0.00500 | Smooth loss: 16.72431\n",
      "[INFO]  Epoch: 01 | Words: 27883238/28022587 | Lr: 0.00500 | Smooth loss: 16.78147\n",
      "[INFO]  Epoch: 01 | Words: 27913505/28022587 | Lr: 0.00500 | Smooth loss: 16.54673\n",
      "[INFO]  Epoch: 01 | Words: 27943487/28022587 | Lr: 0.00500 | Smooth loss: 16.04869\n",
      "[INFO]  Epoch: 01 | Words: 27974213/28022587 | Lr: 0.00500 | Smooth loss: 16.68701\n",
      "[INFO]  Epoch: 01 | Words: 28004526/28022587 | Lr: 0.00500 | Smooth loss: 16.46919\n",
      "Load embeddings ./tmp/Challenge_Dataset/train_transe_model/transe_model_sd_epoch_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "convergence = 0\n",
    "smooth_loss_min = 0\n",
    "\n",
    "\n",
    "def train(dataset='challenge',name='train_transe_model',log_dir = './', device='cuda',seed=123,gpu='0',epochs=1, batch_size=64,lr=0.5,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200):\n",
    "    data = load_dataset(dataset)\n",
    "    dataloader = ChallengeDataLoader(data, batch_size)\n",
    "    words_to_train = epochs * data.text.word_count + 1\n",
    "\n",
    "    model = KnowledgeEmbedding(data, device=device,seed=123,gpu='0',epochs=1, batch_size=64,lr=0.5,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200).to(device)\n",
    "     #logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    steps = 0\n",
    "    smooth_loss = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        dataloader.reset()\n",
    "        while dataloader.has_next():\n",
    "            # Set learning rate.\n",
    "            #lr = lr * max(1e-4, 1.0 - dataloader.finished_word_num / float(words_to_train))\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr\n",
    "\n",
    "            # Get training batch.\n",
    "            batch_idxs = dataloader.get_batch()\n",
    "            batch_idxs = torch.from_numpy(batch_idxs).to(device)\n",
    "\n",
    "            # Train model.\n",
    "            optimizer.zero_grad()\n",
    "            train_loss = model(batch_idxs)\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            smooth_loss += train_loss.item() / steps_per_checkpoint\n",
    "\n",
    "            steps += 1\n",
    "            if steps % steps_per_checkpoint == 0:\n",
    "                logger.info('Epoch: {:02d} | '.format(epoch) +\n",
    "                            'Words: {:d}/{:d} | '.format(dataloader.finished_word_num, words_to_train) +\n",
    "                            'Lr: {:.5f} | '.format(lr) +\n",
    "                            'Smooth loss: {:.5f}'.format(smooth_loss))\n",
    "                smooth_loss = 0.0\n",
    "        \n",
    "        torch.save(model.state_dict(), '{}/transe_model_sd_epoch_{}.ckpt'.format(log_dir, epoch))\n",
    "        \n",
    "\n",
    "def extract_embeddings(dataset='challenge',name='train_transe_model',log_dir='./', seed=123,gpu='0',epochs=1, batch_size=64,lr=0.5,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200):\n",
    "    \"\"\"Note that last entity embedding is of size [vocab_size+1, d].\"\"\"\n",
    "    model_file = '{}/transe_model_sd_epoch_{}.ckpt'.format(log_dir, epochs)\n",
    "    print('Load embeddings', model_file)\n",
    "    state_dict = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "    embeds = {\n",
    "        USER: state_dict['user.weight'].cpu().data.numpy()[:-1],  # Must remove last dummy 'user' with 0 embed.\n",
    "        ARTICLE: state_dict['article.weight'].cpu().data.numpy()[:-1],\n",
    "        WORD: state_dict['word.weight'].cpu().data.numpy()[:-1],\n",
    "        TOPIC: state_dict['topic.weight'].cpu().data.numpy()[:-1],\n",
    "        PRODUCT: state_dict['product.weight'].cpu().data.numpy()[:-1],\n",
    "        RARTICLE: state_dict['related_article.weight'].cpu().data.numpy()[:-1],\n",
    "        TOPIC_TAG: state_dict['topic_tag.weight'].cpu().data.numpy()[:-1],\n",
    "        PRODUCT_TAG: state_dict['product_tag.weight'].cpu().data.numpy()[:-1],\n",
    "        \n",
    "        RECOMMENDED: (\n",
    "            state_dict['recommended'].cpu().data.numpy()[0],\n",
    "            state_dict['recommended_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        WITHIN: (\n",
    "            state_dict['within'].cpu().data.numpy()[0],\n",
    "            state_dict['within_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        HAS_TOPIC: (\n",
    "            state_dict['has_topic'].cpu().data.numpy()[0],\n",
    "            state_dict['has_topic_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        HAS_PRODUCT: (\n",
    "            state_dict['has_product'].cpu().data.numpy()[0],\n",
    "            state_dict['has_product_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        HAS_TOPIC_TAG: (\n",
    "            state_dict['has_topic_tag'].cpu().data.numpy()[0],\n",
    "            state_dict['has_topic_tag_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        HAS_PRODUCT_TAG: (\n",
    "            state_dict['has_product_tag'].cpu().data.numpy()[0],\n",
    "            state_dict['has_product_tag_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        ALSO_RESPONSE: (\n",
    "            state_dict['also_response'].cpu().data.numpy()[0],\n",
    "            state_dict['also_response_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        RECOMMENDED_TOGETHER: (\n",
    "            state_dict['recommended_together'].cpu().data.numpy()[0],\n",
    "            state_dict['recommended_together_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "        RESPONSE_TOGETHER: (\n",
    "            state_dict['response_together'].cpu().data.numpy()[0],\n",
    "            state_dict['response_together_bias.weight'].cpu().data.numpy()\n",
    "        ),\n",
    "    }\n",
    "    save_embed(dataset, embeds)\n",
    "\n",
    "\n",
    "def main(dataset='challenge',name='train_transe_model',seed=123,gpu='0',epochs=1, batch_size=64,lr=0.5,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200):\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    log_dir = '{}/{}'.format(TMP_DIR[dataset], name)\n",
    "    print(log_dir)\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    global logger\n",
    "    logger = get_logger(log_dir + '/train_log.txt')\n",
    "    set_random_seed(seed)\n",
    "    train(dataset='challenge',name='train_transe_model',log_dir= '{}/{}'.format(TMP_DIR[dataset], name),device=device,seed=123,gpu='0',epochs=1, batch_size=64,lr=0.005,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200)\n",
    "    extract_embeddings(dataset='challenge',name='train_transe_model',log_dir=log_dir, seed=123,gpu='0',epochs=1, batch_size=64,lr=0.005,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(dataset='challenge',name='train_transe_model',seed=123,gpu='0',epochs=1, batch_size=64,lr=0.005,weight_decay=0,l2_lambda=0,max_grad_norm=5.0,embed_size=300,num_neg_samples=5,steps_per_checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from kg_env import BatchKGEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Challenge_Dataset/transe_embed.pkl\n",
      "[INFO]  Parameters:['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias', 'actor.weight', 'actor.bias', 'critic.weight', 'critic.bias']\n",
      "[INFO]  Save model to ./tmp/Challenge_Dataset/train_agent/policy_model_epoch_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, gamma=0.99, hidden_sizes=[512, 256]):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, hidden_sizes[0])\n",
    "        self.l2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.actor = nn.Linear(hidden_sizes[1], act_dim)\n",
    "        self.critic = nn.Linear(hidden_sizes[1], 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, act_mask = inputs  # state: [bs, state_dim], act_mask: [bs, act_dim]\n",
    "        x = self.l1(state)\n",
    "        x = F.dropout(F.elu(x), p=0.5)\n",
    "        out = self.l2(x)\n",
    "        x = F.dropout(F.elu(out), p=0.5)\n",
    "\n",
    "        actor_logits = self.actor(x)\n",
    "        actor_logits[1 - act_mask] = -999999.0\n",
    "        act_probs = F.softmax(actor_logits, dim=-1)  # Tensor of [bs, act_dim]\n",
    "\n",
    "        state_values = self.critic(x)  # Tensor of [bs, 1]\n",
    "        return act_probs, state_values\n",
    "\n",
    "    def select_action(self, batch_state, batch_act_mask, device):\n",
    "        state = torch.FloatTensor(batch_state).to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.ByteTensor(batch_act_mask).to(device)  # Tensor of [bs, act_dim]\n",
    "\n",
    "        probs, value = self((state, act_mask))  # act_probs: [bs, act_dim], state_value: [bs, 1]\n",
    "        m = Categorical(probs)\n",
    "        acts = m.sample()  # Tensor of [bs, ], requires_grad=False\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        valid_idx = act_mask.gather(1, acts.view(-1, 1)).view(-1)\n",
    "        acts[valid_idx == 0] = 0\n",
    "\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts), value))\n",
    "        self.entropy.append(m.entropy())\n",
    "        return acts.cpu().numpy().tolist()\n",
    "\n",
    "    def update(self, optimizer, device, ent_weight):\n",
    "        if len(self.rewards) <= 0:\n",
    "            del self.rewards[:]\n",
    "            del self.saved_actions[:]\n",
    "            del self.entropy[:]\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        batch_rewards = np.vstack(self.rewards).T  # numpy array of [bs, #steps]\n",
    "        batch_rewards = torch.FloatTensor(batch_rewards).to(device)\n",
    "        num_steps = batch_rewards.shape[1]\n",
    "        for i in range(1, num_steps):\n",
    "            batch_rewards[:, num_steps - i - 1] += self.gamma * batch_rewards[:, num_steps - i]\n",
    "\n",
    "        actor_loss = 0\n",
    "        critic_loss = 0\n",
    "        entropy_loss = 0\n",
    "        for i in range(0, num_steps):\n",
    "            log_prob, value = self.saved_actions[i]  # log_prob: Tensor of [bs, ], value: Tensor of [bs, 1]\n",
    "            advantage = batch_rewards[:, i] - value.squeeze(1)  # Tensor of [bs, ]\n",
    "            actor_loss += -log_prob * advantage.detach()  # Tensor of [bs, ]\n",
    "            critic_loss += advantage.pow(2)  # Tensor of [bs, ]\n",
    "            entropy_loss += -self.entropy[i]  # Tensor of [bs, ]\n",
    "        actor_loss = actor_loss.mean()\n",
    "        critic_loss = critic_loss.mean()\n",
    "        entropy_loss = entropy_loss.mean()\n",
    "        loss = actor_loss + critic_loss + ent_weight * entropy_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_actions[:]\n",
    "        del self.entropy[:]\n",
    "\n",
    "        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()\n",
    "\n",
    "\n",
    "class ACDataLoader(object):\n",
    "    def __init__(self, uids, batch_size):\n",
    "        self.uids = np.array(uids)\n",
    "        self.num_users = len(uids)\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._rand_perm = np.random.permutation(self.num_users)\n",
    "        self._start_idx = 0\n",
    "        self._has_next = True\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def get_batch(self):\n",
    "        if not self._has_next:\n",
    "            return None\n",
    "        # Multiple users per batch\n",
    "        end_idx = min(self._start_idx + self.batch_size, self.num_users)\n",
    "        batch_idx = self._rand_perm[self._start_idx:end_idx]\n",
    "        batch_uids = self.uids[batch_idx]\n",
    "        self._has_next = self._has_next and end_idx < self.num_users\n",
    "        self._start_idx = end_idx\n",
    "        return batch_uids.tolist()\n",
    "\n",
    "\n",
    "def train( dataset='challenge',name='train_agent',seed=123,gpu='0',epochs=1, batch_size=32,lr=1e-4,max_acts=250,max_path_len=3,\n",
    "    gamma=0.99,ent_weight=1e-3,act_dropout=0.5,state_history=1,hidden=[512, 256],device='cuda',log_dir='./'):\n",
    "    env = BatchKGEnvironment(dataset, max_acts, max_path_len=max_path_len, state_history=state_history)\n",
    "    uids = list(env.kg(USER).keys())\n",
    "    dataloader = ACDataLoader(uids, batch_size)\n",
    "    model = ActorCritic(env.state_dim, env.act_dim, gamma=gamma, hidden_sizes=hidden).to(device)\n",
    "    logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    total_losses, total_plosses, total_vlosses, total_entropy, total_rewards = [], [], [], [], []\n",
    "    step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ### Start epoch ###\n",
    "        dataloader.reset()\n",
    "        while dataloader.has_next():\n",
    "            batch_uids = dataloader.get_batch()\n",
    "            ### Start batch episodes ###\n",
    "            batch_state = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "            done = False\n",
    "            while not done:\n",
    "                batch_act_mask = env.batch_action_mask(dropout=act_dropout)  # numpy array of size [bs, act_dim]\n",
    "                batch_act_idx = model.select_action(batch_state, batch_act_mask, device)  # int\n",
    "                batch_state, batch_reward, done = env.batch_step(batch_act_idx)\n",
    "                model.rewards.append(batch_reward)\n",
    "            ### End of episodes ###\n",
    "\n",
    "            lr_e = lr * max(1e-4, 1.0 - float(step) / (epochs * len(uids) / batch_size))\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_e\n",
    "\n",
    "            # Update policy\n",
    "            total_rewards.append(np.sum(model.rewards))\n",
    "            loss, ploss, vloss, eloss = model.update(optimizer, device, ent_weight)\n",
    "            total_losses.append(loss)\n",
    "            total_plosses.append(ploss)\n",
    "            total_vlosses.append(vloss)\n",
    "            total_entropy.append(eloss)\n",
    "            step += 1\n",
    "\n",
    "            # Report performance\n",
    "            if step > 0 and step % 100 == 0:\n",
    "                avg_reward = np.mean(total_rewards) / batch_size\n",
    "                avg_loss = np.mean(total_losses)\n",
    "                avg_ploss = np.mean(total_plosses)\n",
    "                avg_vloss = np.mean(total_vlosses)\n",
    "                avg_entropy = np.mean(total_entropy)\n",
    "                total_losses, total_plosses, total_vlosses, total_entropy, total_rewards = [], [], [], [], []\n",
    "                logger.info(\n",
    "                        'epoch/step={:d}/{:d}'.format(epoch, step) +\n",
    "                        ' | loss={:.5f}'.format(avg_loss) +\n",
    "                        ' | ploss={:.5f}'.format(avg_ploss) +\n",
    "                        ' | vloss={:.5f}'.format(avg_vloss) +\n",
    "                        ' | entropy={:.5f}'.format(avg_entropy) +\n",
    "                        ' | reward={:.5f}'.format(avg_reward))\n",
    "        ### END of epoch ###\n",
    "\n",
    "        policy_file = '{}/policy_model_epoch_{}.ckpt'.format(log_dir, epoch)\n",
    "        logger.info(\"Save model to \" + policy_file)\n",
    "        torch.save(model.state_dict(), policy_file)\n",
    "\n",
    "\n",
    "def main(dataset='challenge',name='train_agent',seed=123,gpu='0',epochs=1, batch_size=32,lr=1e-4,max_acts=250,max_path_len=3,\n",
    "    gamma=0.99,ent_weight=1e-3,act_dropout=0.5,state_history=1,hidden=[512, 256]):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    log_dir = '{}/{}'.format(TMP_DIR['challenge'], name)\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    global logger\n",
    "    logger = get_logger(log_dir + '/train_log.txt')\n",
    "    #logger.info(args)\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    train(dataset='challenge',name='train_agent',seed=123,gpu='0',epochs=1, batch_size=32,lr=1e-4,max_acts=250,max_path_len=3,\n",
    "    gamma=0.99,ent_weight=1e-3,act_dropout=0.5,state_history=1,hidden=[512, 256],device=device,log_dir=log_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(dataset='challenge',name='train_agent',seed=123,gpu='0',epochs=1, batch_size=32,lr=1e-4,max_acts=250,max_path_len=3,\n",
    "    gamma=0.99,ent_weight=1e-3,act_dropout=0.5,state_history=1,hidden=[512, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from math import log\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import threading\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting paths...\n",
      "Load embedding: ./tmp/Challenge_Dataset/transe_embed.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [00:47, 25.36it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Challenge_Dataset/transe_embed.pkl\n",
      "MAP=19.533 | NDCG=35.850 |  Recall=35.464 | Precision=35.464 | Invalid users=257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [01:00, 25.36it/s]"
     ]
    }
   ],
   "source": [
    "def evaluate(topk_matches, test_user_articles):\n",
    "    \"\"\"Compute metrics for predicted recommendations.\n",
    "    Args:\n",
    "        topk_matches: a list or dict of product ids in ascending order.\n",
    "    \"\"\"\n",
    "    invalid_users = []\n",
    "    # Compute metrics\n",
    "    precisions, recalls, ndcgs, hits, map_scores = [], [], [], [], []\n",
    "    test_user_idxs = list(test_user_articles.keys())\n",
    "    pred_real_articles,pred_l,real_l=[],[],[]\n",
    "    for uid in test_user_idxs:\n",
    "        if uid not in topk_matches or len(topk_matches[uid]) < 10:\n",
    "            invalid_users.append(uid)\n",
    "            continue\n",
    "        pred_list, rel_set = topk_matches[uid][::-1], test_user_articles[uid]\n",
    "        #print(\"uid:\",uid,\"pred_list:\",pred_list,\"real_set:\",rel_set)\n",
    "        pred_real = \"uid:\"+str(uid)+' '+\"pred_list:\"+str(pred_list)+' '+\"rel_set:\"+str(rel_set)\n",
    "        pred_real_articles.append(pred_real)\n",
    "        pred_l.append(pred_list)\n",
    "        real_l.append(rel_set)\n",
    "        #print(pred_real_articles)\n",
    "        if len(pred_list) == 0:\n",
    "            continue\n",
    "        #print(\"uid:\",uid, \"pred_list:\",pred_list, \"rel_set:\",rel_set)\n",
    "        \n",
    "        dcg = 0.0\n",
    "        hit_num = 0.0\n",
    "        for i in range(len(pred_list)):\n",
    "            if pred_list[i] in rel_set:\n",
    "                dcg += 1. / (log(i + 2) / log(2))\n",
    "                hit_num += 1\n",
    "        # idcg\n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(rel_set), len(pred_list))):\n",
    "            idcg += 1. / (log(i + 2) / log(2))\n",
    "        ndcg = dcg / idcg\n",
    "        recall = hit_num / len(rel_set)\n",
    "        precision = hit_num / len(pred_list)\n",
    "        hit = 1.0 if hit_num > 0.0 else 0.0\n",
    "        \n",
    "        #map\n",
    "        map_score = 0.0\n",
    "        num_hits = 0.0\n",
    "        score = 0.0\n",
    "        for i,p in enumerate(pred_list):\n",
    "            if p in rel_set and p not in pred_list[:i]:\n",
    "                num_hits+=1.0\n",
    "                score+=num_hits/(i+1.0)\n",
    "        map_score = score/min(len(rel_set),10)\n",
    "        \n",
    "        ndcgs.append(ndcg)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        hits.append(hit)\n",
    "        map_scores.append(map_score)\n",
    "    \n",
    "    with open(TMP_DIR['challenge'] + '/' +'pred_real_article.dat','wb+') as file:\n",
    "        pickle.dump(pred_real_articles,file)\n",
    "        \n",
    "    with open(TMP_DIR['challenge'] + '/' +'pred_list.dat','wb+') as file:\n",
    "        pickle.dump(pred_l,file)\n",
    "        \n",
    "    with open(TMP_DIR['challenge'] + '/' +'real_list.dat','wb+') as file:\n",
    "        pickle.dump(real_l,file)\n",
    "    \n",
    "    avg_precision = np.mean(precisions) * 100\n",
    "    avg_recall = np.mean(recalls) * 100\n",
    "    avg_ndcg = np.mean(ndcgs) * 100\n",
    "    avg_hit = np.mean(hits) * 100\n",
    "    avg_map = np.mean(map_scores) * 100\n",
    "    \n",
    "    tmp = 'map: '+str(avg_map)+' '+'ndcg: '+str(avg_ndcg)+ ' '+'recall: '+str(avg_recall)+' '+'precision: '+str(avg_precision)+' '+str(len(invalid_users))\n",
    "    pickle.dump(tmp, open(log_dir + '/result.txt', 'wb'))\n",
    "    \n",
    "    print('MAP={:.3f} | NDCG={:.3f} |  Recall={:.3f} | Precision={:.3f}'.format(\n",
    "            avg_map, avg_ndcg, avg_recall, avg_precision))\n",
    "\n",
    "\n",
    "def batch_beam_search(env, model, uids, device, topk=[25, 5, 1]):\n",
    "    def _batch_acts_to_masks(batch_acts):\n",
    "        batch_masks = []\n",
    "        for acts in batch_acts:\n",
    "            num_acts = len(acts)\n",
    "            act_mask = np.zeros(model.act_dim, dtype=np.uint8)\n",
    "            act_mask[:num_acts] = 1\n",
    "            batch_masks.append(act_mask)\n",
    "        return np.vstack(batch_masks)\n",
    "\n",
    "    state_pool = env.reset(uids)  # numpy of [bs, dim]\n",
    "    path_pool = env._batch_path  # list of list, size=bs\n",
    "    probs_pool = [[] for _ in uids]\n",
    "    model.eval()\n",
    "    for hop in range(3):\n",
    "        state_tensor = torch.FloatTensor(state_pool).to(device)\n",
    "        acts_pool = env._batch_get_actions(path_pool, False)  # list of list, size=bs\n",
    "        actmask_pool = _batch_acts_to_masks(acts_pool)  # numpy of [bs, dim]\n",
    "        actmask_tensor = torch.ByteTensor(actmask_pool).to(device)\n",
    "        probs, _ = model((state_tensor, actmask_tensor))  # Tensor of [bs, act_dim]\n",
    "        probs = probs + actmask_tensor.float()  # In order to differ from masked actions\n",
    "        topk_probs, topk_idxs = torch.topk(probs, topk[hop], dim=1)  # LongTensor of [bs, k]\n",
    "        topk_idxs = topk_idxs.detach().cpu().numpy()\n",
    "        topk_probs = topk_probs.detach().cpu().numpy()\n",
    "\n",
    "        new_path_pool, new_probs_pool = [], []\n",
    "        for row in range(topk_idxs.shape[0]):\n",
    "            path = path_pool[row]\n",
    "            probs = probs_pool[row]\n",
    "            for idx, p in zip(topk_idxs[row], topk_probs[row]):\n",
    "                if idx >= len(acts_pool[row]):  # act idx is invalid\n",
    "                    continue\n",
    "                relation, next_node_id = acts_pool[row][idx]  # (relation, next_node_id)\n",
    "                if relation == SELF_LOOP:\n",
    "                    next_node_type = path[-1][1]\n",
    "                else:\n",
    "                    next_node_type = KG_RELATION[path[-1][1]][relation]\n",
    "                new_path = path + [(relation, next_node_type, next_node_id)]\n",
    "                new_path_pool.append(new_path)\n",
    "                new_probs_pool.append(probs + [p])\n",
    "        path_pool = new_path_pool\n",
    "        probs_pool = new_probs_pool\n",
    "        if hop < 2:\n",
    "            state_pool = env._batch_get_state(path_pool)\n",
    "\n",
    "    return path_pool, probs_pool\n",
    "\n",
    "\n",
    "def predict_paths(policy_file, path_file, dataset='challenge',name='train_agent',log_dir='./', device='cuda', seed=123,gpu='0',epochs=1,max_acts=250,max_path_len=5,\n",
    "         gamma=0.99,state_history=1,hidden=[512,256],add_articles=False,topk=[25, 5, 1],run_path=True,run_eval=True):\n",
    "    print('Predicting paths...')\n",
    "    env = BatchKGEnvironment(dataset, max_acts, max_path_len=max_path_len, state_history=state_history)\n",
    "    pretrain_sd = torch.load(policy_file)\n",
    "    #print(env.state_dim,env.act_dim)\n",
    "    model = ActorCritic(env.state_dim, env.act_dim, gamma=gamma, hidden_sizes=hidden).to(device)\n",
    "    model_sd = model.state_dict()\n",
    "    model_sd.update(pretrain_sd)\n",
    "    model.load_state_dict(model_sd)\n",
    "\n",
    "    test_labels = load_labels(dataset, 'test')\n",
    "    test_uids = list(test_labels.keys())\n",
    "\n",
    "    batch_size = 16\n",
    "    start_idx = 0\n",
    "    all_paths, all_probs = [], []\n",
    "    pbar = tqdm(total=len(test_uids))\n",
    "    while start_idx < len(test_uids):\n",
    "        end_idx = min(start_idx + batch_size, len(test_uids))\n",
    "        batch_uids = test_uids[start_idx:end_idx]\n",
    "        paths, probs = batch_beam_search(env, model, batch_uids, device, topk=topk)\n",
    "        all_paths.extend(paths)\n",
    "        all_probs.extend(probs)\n",
    "        start_idx = end_idx\n",
    "        pbar.update(batch_size)\n",
    "    predicts = {'paths': all_paths, 'probs': all_probs}\n",
    "    pickle.dump(predicts, open(path_file, 'wb'))\n",
    "\n",
    "\n",
    "def evaluate_paths(path_file, train_labels, test_labels, add_articles=False):\n",
    "    embeds = load_embed('challenge')\n",
    "    user_embeds = embeds[USER]\n",
    "    response_embeds = embeds[ARTICLE][0]\n",
    "    article_embeds = embeds[ARTICLE]\n",
    "    scores = np.dot(user_embeds + response_embeds, article_embeds.T)\n",
    "\n",
    "    # 1) Get all valid paths for each user, compute path score and path probability.\n",
    "    results = pickle.load(open(path_file, 'rb'))\n",
    "    #print(\"result_path:\",results['paths'])\n",
    "    pred_paths = {uid: {} for uid in test_labels}\n",
    "    for path, probs in zip(results['paths'], results['probs']):\n",
    "        if path[-1][1] != ARTICLE:\n",
    "            continue\n",
    "        uid = path[0][2]\n",
    "        if uid not in pred_paths:\n",
    "            continue\n",
    "        aid = path[-1][2]\n",
    "        if aid not in pred_paths[uid]:\n",
    "            pred_paths[uid][aid] = []\n",
    "        path_score = scores[uid][aid]\n",
    "        path_prob = reduce(lambda x, y: x * y, probs)\n",
    "        pred_paths[uid][aid].append((path_score, path_prob, path))\n",
    "    \n",
    "    # 2) Pick best path for each user-product pair, also remove pid if it is in train set.\n",
    "    best_pred_paths = {}\n",
    "    for uid in pred_paths:\n",
    "        if uid in train_labels:\n",
    "            train_aids = set(train_labels[uid])\n",
    "            best_pred_paths[uid] = []\n",
    "            for aid in pred_paths[uid]:\n",
    "                if aid in train_aids:\n",
    "                    continue\n",
    "                # Get the path with highest probability\n",
    "                #print(\"pred_path:\",pred_paths)\n",
    "                sorted_path = sorted(pred_paths[uid][aid], key=lambda x: x[1], reverse=True)\n",
    "                best_pred_paths[uid].append(sorted_path[0])\n",
    "    #print(\"best_pred_path:\",best_pred_paths)\n",
    "    \n",
    "    with open(TMP_DIR['challenge'] + '/' +'best_pred_path.dat','wb+') as file:\n",
    "        pickle.dump(best_pred_paths,file)\n",
    "    \n",
    "    # 3) Compute top 10 recommended articls for each user.\n",
    "    sort_by = 'score'\n",
    "    pred_labels = {}\n",
    "    for uid in best_pred_paths:\n",
    "        if sort_by == 'score':\n",
    "            sorted_path = sorted(best_pred_paths[uid], key=lambda x: (x[0], x[1]), reverse=True)\n",
    "        elif sort_by == 'prob':\n",
    "            sorted_path = sorted(best_pred_paths[uid], key=lambda x: (x[1], x[0]), reverse=True)\n",
    "        top10_aids = [p[-1][2] for _, _, p in sorted_path[:10]]  # from largest to smallest\n",
    "        # add up to 10 pids if not enough\n",
    "        if add_articles and len(top10_aids) < 10:\n",
    "            train_aids = set(train_labels[uid])\n",
    "            cand_aids = np.argsort(scores[uid])\n",
    "            for cand_aid in cand_aids[::-1]:\n",
    "                if cand_aid in train_aids or cand_aid in top10_aids:\n",
    "                    continue\n",
    "                top10_aids.append(cand_aid)\n",
    "                if len(top10_aids) >= 10:\n",
    "                    break\n",
    "        # end of add\n",
    "        pred_labels[uid] = top10_aids[::-1]  # change order to from smallest to largest!\n",
    "    evaluate(pred_labels, test_labels)\n",
    "\n",
    "\n",
    "def test(dataset='challenge',name='train_agent',log_dir='./', device='cuda', seed=123,gpu='0',epochs=1,max_acts=250,max_path_len=5,\n",
    "         gamma=0.99,state_history=1,hidden=[512,256],add_articles=False,topk=[25, 5, 1],run_path=True,run_eval=True):\n",
    "    \n",
    "    policy_file = log_dir + '/policy_model_epoch_{}.ckpt'.format(epochs)\n",
    "    path_file = log_dir + '/policy_paths_epoch{}.pkl'.format(epochs)\n",
    "\n",
    "    train_labels = load_labels(dataset, 'train')\n",
    "    test_labels = load_labels(dataset, 'test')\n",
    "    \n",
    "    if run_path:\n",
    "        predict_paths(policy_file, path_file, dataset='challenge',name='train_agent',log_dir='./', device='cuda', seed=123,gpu='0',epochs=1,max_acts=250,max_path_len=5,\n",
    "         gamma=0.99,state_history=1,hidden=[512,256],add_articles=False,topk=[25, 5, 1],run_path=True,run_eval=True)\n",
    "    if run_eval:\n",
    "        evaluate_paths(path_file, train_labels, test_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    boolean = lambda x: (str(x).lower() == 'true')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    log_dir = TMP_DIR['challenge'] + '/' + 'train_agent'\n",
    "    test(dataset='challenge',name='train_agent',log_dir=log_dir, device=device, seed=123,gpu='0',epochs=1,max_acts=250,max_path_len=5,\n",
    "         gamma=0.99,state_history=1,hidden=[512,256],add_articles=False,topk=[25, 5, 1],run_path=True,run_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:647324198242:studio-lifecycle-config/dlsg-sagemaker-kernel-on-start-e8130f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
