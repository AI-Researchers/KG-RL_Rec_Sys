{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import gzip\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket='dlsg-training-data-prod'\n",
    "prefix = 'a708148/' \n",
    "kms_key = \"arn:aws:kms:us-east-1:647324198242:alias/ap131630-sagemaker\"\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3_input      = 's3://'+bucket+'/'+prefix+'Rec_sys_challenge/original_dataset/'\n",
    "s3_output_location  = 's3://'+bucket+'/'+prefix+'Rec_sys_challenge/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = sagemaker.s3.S3Downloader.read_file('s3://dlsg-training-data-prod/a708148/Rec_sys_challenge/original_dataset/articles.csv', sagemaker_session)\n",
    "articles = pd.read_csv(io.StringIO(articles), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sagemaker.s3.S3Downloader.read_file('s3://dlsg-training-data-prod/a708148/Rec_sys_challenge/original_dataset/users_v3.csv', sagemaker_session)\n",
    "users = pd.read_csv(io.StringIO(users), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sagemaker.s3.S3Downloader.read_file('s3://dlsg-training-data-prod/a708148/Rec_sys_challenge/original_dataset/Rec_Sys_sample_train_data_v3.csv', sagemaker_session)\n",
    "df = pd.read_csv(io.StringIO(df), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id=articles['tcm_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/Challenge_Dataset/articles.txt',article_id,fmt=\"%s\")\n",
    "np.savetxt('./data/Challenge_Dataset/related_article.txt',article_id,fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "topics = articles['topic'].dropna()\n",
    "for topic in topics:\n",
    "    t = topic.strip('[').strip(']').strip().split(',')\n",
    "    for i in range(len(t)):\n",
    "        j = t[i].strip().replace(\"'\",'')\n",
    "        if j not in topic_list:\n",
    "            topic_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/topics.txt\",'w') as f:\n",
    "    for i in topic_list:\n",
    "        f.write(i+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tag_list = []\n",
    "topic_tags = articles['vstopic'].dropna()\n",
    "for tag in topic_tags:\n",
    "    t = tag.strip('[').strip(']').strip().split(',')\n",
    "    for i in range(len(t)):\n",
    "        j = t[i].strip().replace(\"'\",'')\n",
    "        if j not in topic_tag_list:\n",
    "            topic_tag_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/topic_tags.txt\",'w') as f:\n",
    "    for i in topic_tag_list:\n",
    "        f.write(i+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsproduct_list = []\n",
    "products = articles['vsproduct'].dropna()\n",
    "for product in products:\n",
    "    t = product.strip('[').strip(']').strip().split(',')\n",
    "    for i in range(len(t)):\n",
    "        j = t[i].strip().replace(\"'\",'')\n",
    "        if j not in vsproduct_list:\n",
    "            vsproduct_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = []\n",
    "products = articles['product'].dropna()\n",
    "for product in products:\n",
    "    t = product.strip('[').strip(']').strip().split(',')\n",
    "    for i in range(len(t)):\n",
    "        j = t[i].strip().replace(\"'\",'')\n",
    "        if j not in product_list:\n",
    "            product_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/products.txt\",'w') as f:\n",
    "    for i in product_list:\n",
    "        f.write(i+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/product_tags.txt\",'w') as f:\n",
    "    for i in vsproduct_list:\n",
    "        f.write(i+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = df['ip'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/Challenge_Dataset/users.txt',user_id,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in user_id:\n",
    "    d[i]=list(df[df['ip']==i]['tcm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_together = []\n",
    "for a_id in article_id:\n",
    "    tmp = {}\n",
    "    for k,v in d.items():\n",
    "        if a_id in tmp:\n",
    "            if a_id in v:\n",
    "                tmp[a_id]=list(set(tmp[a_id]).intersection(set(v)))\n",
    "        else:\n",
    "            if a_id in v:\n",
    "                tmp[a_id]=v\n",
    "            else:\n",
    "                tmp[a_id]=[]\n",
    "    recommended_together.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tcm:526-10787': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-10787']},\n",
       " {'tcm:526-10838': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-10838']},\n",
       " {'tcm:526-10849': ['tcm:526-10849', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-116442': ['tcm:526-116442', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-12167': ['tcm:526-12167', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-12173': []},\n",
       " {'tcm:526-12174': ['tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-12180': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-12180']},\n",
       " {'tcm:526-13782': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-13782']},\n",
       " {'tcm:526-149043': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-149043']},\n",
       " {'tcm:526-153236': ['tcm:526-153236', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-161967': []},\n",
       " {'tcm:526-164352': ['tcm:526-164352',\n",
       "   'tcm:526-12174',\n",
       "   'tcm:526-208086',\n",
       "   'tcm:526-22867']},\n",
       " {'tcm:526-16645': ['tcm:526-16645', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-18025': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-18025']},\n",
       " {'tcm:526-187302': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-187302']},\n",
       " {'tcm:526-191289': []},\n",
       " {'tcm:526-199040': []},\n",
       " {'tcm:526-204679': ['tcm:526-204679', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-208086': ['tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-209028': ['tcm:526-209028', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-209216': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-209216']},\n",
       " {'tcm:526-224772': []},\n",
       " {'tcm:526-228258': ['tcm:526-228258', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-22867': ['tcm:526-22867', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-231792': ['tcm:526-208086', 'tcm:526-12174', 'tcm:526-231792']},\n",
       " {'tcm:526-237984': []},\n",
       " {'tcm:526-239640': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-239640']},\n",
       " {'tcm:526-244935': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-244935']},\n",
       " {'tcm:526-248966': []},\n",
       " {'tcm:526-249037': ['tcm:526-116442',\n",
       "   'tcm:526-12174',\n",
       "   'tcm:526-208086',\n",
       "   'tcm:526-249037']},\n",
       " {'tcm:526-249921': ['tcm:526-208086', 'tcm:526-12174', 'tcm:526-249921']},\n",
       " {'tcm:526-258965': ['tcm:526-116442',\n",
       "   'tcm:526-12174',\n",
       "   'tcm:526-208086',\n",
       "   'tcm:526-258965']},\n",
       " {'tcm:526-259315': ['tcm:526-12174', 'tcm:526-259315', 'tcm:526-208086']},\n",
       " {'tcm:526-261006': ['tcm:526-208086', 'tcm:526-12174', 'tcm:526-261006']},\n",
       " {'tcm:526-273898': []},\n",
       " {'tcm:526-276414': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-276414']},\n",
       " {'tcm:526-346380': ['tcm:526-346380', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-385364': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-385364']},\n",
       " {'tcm:526-387709': []},\n",
       " {'tcm:526-388698': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-388698']},\n",
       " {'tcm:526-40469': []},\n",
       " {'tcm:526-418532': ['tcm:526-418532', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-46242': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-46242']},\n",
       " {'tcm:526-492865': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-492865']},\n",
       " {'tcm:526-52560': ['tcm:526-52560', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-551763': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-551763']},\n",
       " {'tcm:526-635549': []},\n",
       " {'tcm:526-653510': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-653510']},\n",
       " {'tcm:526-654768': []},\n",
       " {'tcm:526-655038': ['tcm:526-655038', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-656536': ['tcm:526-656536', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-670542': ['tcm:526-12174', 'tcm:526-670542', 'tcm:526-208086']},\n",
       " {'tcm:526-674187': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-674187']},\n",
       " {'tcm:526-679101': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-679101']},\n",
       " {'tcm:526-681575': ['tcm:526-681575', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-682379': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-682379']},\n",
       " {'tcm:526-682576': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-682576']},\n",
       " {'tcm:526-684643': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-684643']},\n",
       " {'tcm:526-688510': ['tcm:526-688510', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-695181': ['tcm:526-695181', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-704480': ['tcm:526-704480', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-707527': ['tcm:526-707527', 'tcm:526-12174', 'tcm:526-208086']},\n",
       " {'tcm:526-707567': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-707567']},\n",
       " {'tcm:526-70899': []},\n",
       " {'tcm:526-712788': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-712788']},\n",
       " {'tcm:526-766928': []},\n",
       " {'tcm:526-782343': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-782343']},\n",
       " {'tcm:526-784445': []},\n",
       " {'tcm:526-80807': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-80807']},\n",
       " {'tcm:526-93195': ['tcm:526-12174', 'tcm:526-208086', 'tcm:526-93195']}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_response_article = {}\n",
    "for i in user_id:\n",
    "    d_response_article[i]=list(df[(df['ip']==i) & (df['response']==1)]['tcm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_response = {}\n",
    "for a_id in article_id:\n",
    "    l = []\n",
    "    for k,v in d_response_article.items():\n",
    "        if a_id in v:\n",
    "            l+=v\n",
    "    also_response[a_id]=list(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_together = []\n",
    "for a_id in article_id:\n",
    "    tmp = {}\n",
    "    for k,v in d_response_article.items():\n",
    "        if a_id in tmp:\n",
    "            if a_id in v:\n",
    "                tmp[a_id]=list(set(tmp[a_id]).intersection(set(v)))\n",
    "        else:\n",
    "            if a_id in v:\n",
    "                tmp[a_id] = v\n",
    "            else:\n",
    "                tmp[a_id] = []\n",
    "    response_together.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_dict_generate(text):\n",
    "    word_dict = {}\n",
    "    words = []\n",
    "    for line in text:\n",
    "        line = line.strip().lower()\n",
    "        #line = re.sub(r\"[^A-Za-z0-9()<>:;,.!?\\'\\\"]\",\" \",line)\n",
    "        line = re.sub(r\",\",\" \",line)\n",
    "        line = re.sub(r\";\",\" \",line)\n",
    "        line = re.sub(r\"\\'\",\" \",line)\n",
    "        line = re.sub(r\":\",\" \",line)\n",
    "        line = re.sub(r\"!\",\" \",line)\n",
    "        line = re.sub(r\"\\.\",\" \",line)\n",
    "        line = re.sub(r\"\\/\",\" \",line)\n",
    "        line = re.sub(r\"\\?\",\" \", line)\n",
    "        line = re.sub(r\"[\\\\d]\",\" \",line)\n",
    "        line = re.sub(r\"\\'\\'\",\" \",line)\n",
    "        line = re.sub(r\"\\(\",\" \",line)\n",
    "        line = re.sub(r\"\\)\",\" \",line)\n",
    "        line = re.sub(r\"\\<\",\" \",line)\n",
    "        line = re.sub(r\"\\>\",\" \",line)\n",
    "        line = nltk.word_tokenize(line)\n",
    "        for w in line:\n",
    "            if w not in stopwords.words(\"english\") and len(w)>3:\n",
    "                if w not in word_dict:\n",
    "                    word_dict[w]=1\n",
    "                else:\n",
    "                    word_dict[w]+=1\n",
    "                words.append(w)\n",
    "        #words+=line\n",
    "    return word_dict,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_count_dict,words = word_dict_generate(articles['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5359, 49263)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count_dict),len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, words):\n",
    "    tfDict = {}\n",
    "    wordsCount = len(words)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(wordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = computeTF(word_count_dict,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(text):\n",
    "    N = len(text)\n",
    "    idfDict = dict.fromkeys(text.keys(), 0)\n",
    "    for word, val in text.items():\n",
    "        if val > 0:\n",
    "            idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        if val!=0:\n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = computeIDF(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfDict, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfDict.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = computeTFIDF(tf,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for w,s in tfidf.items():\n",
    "    if s>0.0005:\n",
    "        word_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/vocab.txt\",'w') as f:\n",
    "    for w in word_list:\n",
    "        f.write(w+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_set(s):\n",
    "    i = 0\n",
    "    s_map = {}\n",
    "    for key in s:\n",
    "        s_map[key] = str(i)\n",
    "        i+=1\n",
    "    return s_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = index_set(user_id)\n",
    "article_map = index_set(article_id)\n",
    "word_map = index_set(word_list)\n",
    "topic_map = index_set(topic_list)\n",
    "tag_map = index_set(topic_tag_list)\n",
    "product_map = index_set(product_list)\n",
    "topic_tag_map = index_set(topic_tag_list)\n",
    "product_tag_map = index_set(vsproduct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/response_together.txt\",'w') as f:\n",
    "    for line in response_together:\n",
    "        t = ''\n",
    "        for k,v in line.items():\n",
    "            for k in v:\n",
    "                t=t+' '+article_map[k]\n",
    "        f.write(article_map[k]+' '+t+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/recommended_together.txt\",'w') as f:\n",
    "    for line in recommended_together:\n",
    "        t = ''\n",
    "        for k,v in line.items():\n",
    "            for k in v:\n",
    "                t=t+' '+article_map[k]\n",
    "        f.write(article_map[k]+' '+t+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/also_response.txt\",'w') as f:\n",
    "    for k,v in also_response.items():\n",
    "        t = ''\n",
    "        for k in v:\n",
    "            t=t+' '+article_map[k]\n",
    "        f.write(article_map[k]+' '+t+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/has_topic.txt\",'w') as f:\n",
    "    for index,row in articles.iterrows():\n",
    "        g = ''\n",
    "        if type(row['topic']) == str:\n",
    "            topics = row['topic'].strip('[').strip(']').strip().split(',')\n",
    "            for t in topics:\n",
    "                s = t.strip().replace(\"'\",'')\n",
    "                g=g+' '+topic_map[s]\n",
    "        f.write(g+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has_vstopic_tag\n",
    "with open(\"./data/Challenge_Dataset/has_topic_tag.txt\",'w') as f:\n",
    "    for index,row in articles.iterrows():\n",
    "        k = ''\n",
    "        if type(row['vstopic']) == str:\n",
    "            tags = row['vstopic'].strip('[').strip(']').strip().split(',')\n",
    "            for t in tags:\n",
    "                s = t.strip().replace(\"'\",'')\n",
    "                k=k+' '+topic_tag_map[s]\n",
    "        #f.write(article_map[row['tcm_id']]+' '+k+'\\n')\n",
    "        f.write(k+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has_vsproduct_tag\n",
    "with open(\"./data/Challenge_Dataset/has_product_tag.txt\",'w') as f:\n",
    "    for index,row in articles.iterrows():\n",
    "        k = ''\n",
    "        if type(row['vsproduct']) == str:\n",
    "            tags = row['vsproduct'].strip('[').strip(']').strip().split(',')\n",
    "            for t in tags:\n",
    "                s = t.strip().replace(\"'\",'')\n",
    "                k=k+' '+product_tag_map[s]\n",
    "        #f.write(article_map[row['tcm_id']]+' '+k+'\\n')\n",
    "        f.write(k+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/has_product.txt\",'w') as f:\n",
    "    for index,row in articles.iterrows():\n",
    "        g = ''\n",
    "        #print(article_map[row['tcm_id']]+' '+\n",
    "        if type(row['product']) == str:\n",
    "            products = row['product'].strip('[').strip(']').strip().split(',')\n",
    "            for t in products:\n",
    "                s = t.strip().replace(\"'\",'')\n",
    "                g=g+' '+product_map[s]\n",
    "        f.write(g+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/Challenge_Dataset/dataset.txt\",'w') as f:\n",
    "    for index,row in df.iterrows():\n",
    "        if len(articles[articles['tcm_id']==row['tcm_id']]['text'].values) > 0:\n",
    "            text = articles[articles['tcm_id']==row['tcm_id']]['text'].values[0].strip().lower().split(' ')\n",
    "            k = ''\n",
    "            for w in text:\n",
    "                if w in word_map:\n",
    "                    k=k+' '+word_map[w]\n",
    "            f.write(user_map[row['ip']]+'\\t'+article_map[row['tcm_id']]+'\\t'+k+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each customer has 10 articles in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with open('./data/Challenge_Dataset/dataset.txt','r') as f:\n",
    "    i = 0\n",
    "    t = f.readlines()\n",
    "    j = len(t)\n",
    "    l_train,l_test = [],[]\n",
    "    \n",
    "    d={}\n",
    "    for line in t:\n",
    "        c = line.split('\\t')[0]\n",
    "        if c in d:\n",
    "            if len(d[c])<10:\n",
    "                d[c].append(line)\n",
    "                l_test.append(line)\n",
    "            else:\n",
    "                l_train.append(line)\n",
    "        else:\n",
    "            d[c]=[line]\n",
    "            l_test.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Challenge_Dataset/train.txt','w') as f:\n",
    "    for l in l_train:\n",
    "        f.write(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Challenge_Dataset/test.txt','w') as f:\n",
    "    for l in l_test:\n",
    "        f.write(l)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:647324198242:studio-lifecycle-config/dlsg-sagemaker-kernel-on-start-d120ff"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
